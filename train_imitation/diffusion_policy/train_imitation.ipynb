{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "success_rate, time_taken, distance_travelled, straight_distance, (optimal_time or dijsktra_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success</th>\n",
       "      <th>actual_time</th>\n",
       "      <th>optimal_time</th>\n",
       "      <th>world_idx</th>\n",
       "      <th>timestep</th>\n",
       "      <th>lidar_0</th>\n",
       "      <th>lidar_1</th>\n",
       "      <th>lidar_2</th>\n",
       "      <th>lidar_3</th>\n",
       "      <th>lidar_4</th>\n",
       "      <th>...</th>\n",
       "      <th>lidar_719</th>\n",
       "      <th>pos_x</th>\n",
       "      <th>pos_y</th>\n",
       "      <th>pose_heading</th>\n",
       "      <th>twist_linear</th>\n",
       "      <th>twist_angular</th>\n",
       "      <th>cmd_vel_linear</th>\n",
       "      <th>cmd_vel_angular</th>\n",
       "      <th>local_goal_x</th>\n",
       "      <th>local_goal_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.970249</td>\n",
       "      <td>2.952848</td>\n",
       "      <td>2.942288</td>\n",
       "      <td>2.937646</td>\n",
       "      <td>2.942004</td>\n",
       "      <td>...</td>\n",
       "      <td>2.974557</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>1.571106</td>\n",
       "      <td>0.054851</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.970249</td>\n",
       "      <td>2.952848</td>\n",
       "      <td>2.942288</td>\n",
       "      <td>2.937646</td>\n",
       "      <td>2.942004</td>\n",
       "      <td>...</td>\n",
       "      <td>2.974500</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>1.571106</td>\n",
       "      <td>0.054851</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.970249</td>\n",
       "      <td>2.952848</td>\n",
       "      <td>2.942288</td>\n",
       "      <td>2.937646</td>\n",
       "      <td>2.942004</td>\n",
       "      <td>...</td>\n",
       "      <td>2.974500</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>1.571106</td>\n",
       "      <td>0.054851</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.970935</td>\n",
       "      <td>2.952720</td>\n",
       "      <td>2.942559</td>\n",
       "      <td>2.940268</td>\n",
       "      <td>2.943001</td>\n",
       "      <td>...</td>\n",
       "      <td>2.972595</td>\n",
       "      <td>-0.000417</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>1.571247</td>\n",
       "      <td>0.060888</td>\n",
       "      <td>0.008210</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.969906</td>\n",
       "      <td>2.953190</td>\n",
       "      <td>2.943104</td>\n",
       "      <td>2.942191</td>\n",
       "      <td>2.944316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.972554</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>1.571141</td>\n",
       "      <td>0.066950</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 734 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   success  actual_time  optimal_time  world_idx  timestep   lidar_0  \\\n",
       "0     True       12.367      6.796149          0         0  2.970249   \n",
       "1     True       12.367      6.796149          0         1  2.970249   \n",
       "2     True       12.367      6.796149          0         2  2.970249   \n",
       "3     True       12.367      6.796149          0         3  2.970935   \n",
       "4     True       12.367      6.796149          0         4  2.969906   \n",
       "\n",
       "    lidar_1   lidar_2   lidar_3   lidar_4  ...  lidar_719     pos_x     pos_y  \\\n",
       "0  2.952848  2.942288  2.937646  2.942004  ...   2.974557 -0.000416  0.006752   \n",
       "1  2.952848  2.942288  2.937646  2.942004  ...   2.974500 -0.000416  0.006752   \n",
       "2  2.952848  2.942288  2.937646  2.942004  ...   2.974500 -0.000416  0.006752   \n",
       "3  2.952720  2.942559  2.940268  2.943001  ...   2.972595 -0.000417  0.008014   \n",
       "4  2.953190  2.943104  2.942191  2.944316  ...   2.972554 -0.000418  0.009397   \n",
       "\n",
       "   pose_heading  twist_linear  twist_angular  cmd_vel_linear  cmd_vel_angular  \\\n",
       "0      1.571106      0.054851      -0.000015           0.066         0.006651   \n",
       "1      1.571106      0.054851      -0.000015           0.066         0.007315   \n",
       "2      1.571106      0.054851      -0.000015           0.066         0.007315   \n",
       "3      1.571247      0.060888       0.008210           0.072         0.007980   \n",
       "4      1.571141      0.066950      -0.001348           0.078         0.008645   \n",
       "\n",
       "   local_goal_x  local_goal_y  \n",
       "0         -0.05          0.35  \n",
       "1         -0.05          0.35  \n",
       "2         -0.05          0.35  \n",
       "3         -0.05          0.35  \n",
       "4         -0.05          0.35  \n",
       "\n",
       "[5 rows x 734 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data_sorted.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_time</th>\n",
       "      <th>optimal_time</th>\n",
       "      <th>world_idx</th>\n",
       "      <th>timestep</th>\n",
       "      <th>lidar_0</th>\n",
       "      <th>lidar_1</th>\n",
       "      <th>lidar_2</th>\n",
       "      <th>lidar_3</th>\n",
       "      <th>lidar_4</th>\n",
       "      <th>lidar_5</th>\n",
       "      <th>...</th>\n",
       "      <th>lidar_719</th>\n",
       "      <th>pos_x</th>\n",
       "      <th>pos_y</th>\n",
       "      <th>pose_heading</th>\n",
       "      <th>twist_linear</th>\n",
       "      <th>twist_angular</th>\n",
       "      <th>cmd_vel_linear</th>\n",
       "      <th>cmd_vel_angular</th>\n",
       "      <th>local_goal_x</th>\n",
       "      <th>local_goal_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.731146</td>\n",
       "      <td>5.705953</td>\n",
       "      <td>151.868460</td>\n",
       "      <td>340.027182</td>\n",
       "      <td>2.977978</td>\n",
       "      <td>2.973894</td>\n",
       "      <td>2.970005</td>\n",
       "      <td>2.966131</td>\n",
       "      <td>2.962435</td>\n",
       "      <td>2.956835</td>\n",
       "      <td>...</td>\n",
       "      <td>2.974177</td>\n",
       "      <td>-0.031480</td>\n",
       "      <td>4.134166</td>\n",
       "      <td>1.554468</td>\n",
       "      <td>0.709279</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.710038</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>-0.048144</td>\n",
       "      <td>4.451120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.094962</td>\n",
       "      <td>0.366456</td>\n",
       "      <td>86.482099</td>\n",
       "      <td>205.708619</td>\n",
       "      <td>2.099984</td>\n",
       "      <td>2.112733</td>\n",
       "      <td>2.124795</td>\n",
       "      <td>2.136720</td>\n",
       "      <td>2.148487</td>\n",
       "      <td>2.159736</td>\n",
       "      <td>...</td>\n",
       "      <td>2.181376</td>\n",
       "      <td>0.524510</td>\n",
       "      <td>2.795738</td>\n",
       "      <td>0.346113</td>\n",
       "      <td>0.205768</td>\n",
       "      <td>0.248443</td>\n",
       "      <td>0.202546</td>\n",
       "      <td>0.219348</td>\n",
       "      <td>0.536361</td>\n",
       "      <td>2.801547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.778000</td>\n",
       "      <td>5.026614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316017</td>\n",
       "      <td>0.312385</td>\n",
       "      <td>0.308484</td>\n",
       "      <td>0.304327</td>\n",
       "      <td>0.302636</td>\n",
       "      <td>0.299997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291395</td>\n",
       "      <td>-1.710844</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-3.141033</td>\n",
       "      <td>-0.317507</td>\n",
       "      <td>-2.412546</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-1.570796</td>\n",
       "      <td>-1.814288</td>\n",
       "      <td>0.293437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.923000</td>\n",
       "      <td>5.441711</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>1.693966</td>\n",
       "      <td>1.688156</td>\n",
       "      <td>1.682930</td>\n",
       "      <td>1.677800</td>\n",
       "      <td>1.673200</td>\n",
       "      <td>1.668052</td>\n",
       "      <td>...</td>\n",
       "      <td>1.612400</td>\n",
       "      <td>-0.294785</td>\n",
       "      <td>1.631851</td>\n",
       "      <td>1.467639</td>\n",
       "      <td>0.799555</td>\n",
       "      <td>-0.059794</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.065792</td>\n",
       "      <td>-0.325890</td>\n",
       "      <td>1.919969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.121000</td>\n",
       "      <td>5.612314</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>2.879627</td>\n",
       "      <td>2.865375</td>\n",
       "      <td>2.851746</td>\n",
       "      <td>2.838104</td>\n",
       "      <td>2.824749</td>\n",
       "      <td>2.811204</td>\n",
       "      <td>...</td>\n",
       "      <td>2.844913</td>\n",
       "      <td>-0.004195</td>\n",
       "      <td>4.014582</td>\n",
       "      <td>1.579035</td>\n",
       "      <td>0.800235</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.502000</td>\n",
       "      <td>5.891979</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>3.187622</td>\n",
       "      <td>3.172247</td>\n",
       "      <td>3.157621</td>\n",
       "      <td>3.144867</td>\n",
       "      <td>3.130252</td>\n",
       "      <td>3.113974</td>\n",
       "      <td>...</td>\n",
       "      <td>3.177820</td>\n",
       "      <td>0.217379</td>\n",
       "      <td>6.539444</td>\n",
       "      <td>1.680124</td>\n",
       "      <td>0.801052</td>\n",
       "      <td>0.044571</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.052247</td>\n",
       "      <td>0.231511</td>\n",
       "      <td>6.875039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.666000</td>\n",
       "      <td>6.867653</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>1328.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.690635</td>\n",
       "      <td>9.312726</td>\n",
       "      <td>3.140765</td>\n",
       "      <td>0.810490</td>\n",
       "      <td>2.206750</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.570796</td>\n",
       "      <td>1.689592</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 733 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         actual_time   optimal_time      world_idx       timestep  \\\n",
       "count  200540.000000  200540.000000  200540.000000  200540.000000   \n",
       "mean       12.731146       5.705953     151.868460     340.027182   \n",
       "std         2.094962       0.366456      86.482099     205.708619   \n",
       "min        11.778000       5.026614       0.000000       0.000000   \n",
       "25%        11.923000       5.441711      78.000000     167.000000   \n",
       "50%        12.121000       5.612314     153.000000     335.000000   \n",
       "75%        12.502000       5.891979     227.000000     503.000000   \n",
       "max        25.666000       6.867653     299.000000    1328.000000   \n",
       "\n",
       "             lidar_0        lidar_1        lidar_2        lidar_3  \\\n",
       "count  200540.000000  200540.000000  200540.000000  200540.000000   \n",
       "mean        2.977978       2.973894       2.970005       2.966131   \n",
       "std         2.099984       2.112733       2.124795       2.136720   \n",
       "min         0.316017       0.312385       0.308484       0.304327   \n",
       "25%         1.693966       1.688156       1.682930       1.677800   \n",
       "50%         2.879627       2.865375       2.851746       2.838104   \n",
       "75%         3.187622       3.172247       3.157621       3.144867   \n",
       "max        10.000000      10.000000      10.000000      10.000000   \n",
       "\n",
       "             lidar_4        lidar_5  ...      lidar_719          pos_x  \\\n",
       "count  200540.000000  200540.000000  ...  200540.000000  200540.000000   \n",
       "mean        2.962435       2.956835  ...       2.974177      -0.031480   \n",
       "std         2.148487       2.159736  ...       2.181376       0.524510   \n",
       "min         0.302636       0.299997  ...       0.291395      -1.710844   \n",
       "25%         1.673200       1.668052  ...       1.612400      -0.294785   \n",
       "50%         2.824749       2.811204  ...       2.844913      -0.004195   \n",
       "75%         3.130252       3.113974  ...       3.177820       0.217379   \n",
       "max        10.000000      10.000000  ...      10.000000       1.690635   \n",
       "\n",
       "               pos_y   pose_heading   twist_linear  twist_angular  \\\n",
       "count  200540.000000  200540.000000  200540.000000  200540.000000   \n",
       "mean        4.134166       1.554468       0.709279      -0.001472   \n",
       "std         2.795738       0.346113       0.205768       0.248443   \n",
       "min        -0.000102      -3.141033      -0.317507      -2.412546   \n",
       "25%         1.631851       1.467639       0.799555      -0.059794   \n",
       "50%         4.014582       1.579035       0.800235      -0.000544   \n",
       "75%         6.539444       1.680124       0.801052       0.044571   \n",
       "max         9.312726       3.140765       0.810490       2.206750   \n",
       "\n",
       "       cmd_vel_linear  cmd_vel_angular   local_goal_x   local_goal_y  \n",
       "count   200540.000000    200540.000000  200540.000000  200540.000000  \n",
       "mean         0.710038        -0.001961      -0.048144       4.451120  \n",
       "std          0.202546         0.219348       0.536361       2.801547  \n",
       "min         -0.300000        -1.570796      -1.814288       0.293437  \n",
       "25%          0.800000        -0.065792      -0.325890       1.919969  \n",
       "50%          0.800000         0.000088      -0.050000       4.300000  \n",
       "75%          0.800000         0.052247       0.231511       6.875039  \n",
       "max          0.800000         1.570796       1.689592      10.000000  \n",
       "\n",
       "[8 rows x 733 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch Dataset\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class KULBarnDataset(Dataset):\n",
    "    def get_local_goal(self):\n",
    "        x = self.data['pos_x']\n",
    "        y = self.data['pos_y']\n",
    "        theta = self.data['pose_heading']\n",
    "        goal_x = self.data['local_goal_x']\n",
    "        goal_y = self.data['local_goal_y']\n",
    "        self.data['local_x'] = (goal_x - x) * np.cos(theta) + (goal_y - y) * np.sin(theta)\n",
    "        self.data['local_y'] = -(goal_x - x) * np.sin(theta) + (goal_y - y) * np.cos(theta)\n",
    "        self.data['distance'] = np.sqrt((goal_x - x)**2 + (goal_y - y)**2)\n",
    "        self.data['local_x'] /= self.data['distance']\n",
    "        self.data['local_y'] /= self.data['distance']\n",
    "    \n",
    "    def __init__(self, df, mode=\"train\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = df\n",
    "        self.get_local_goal()  \n",
    "        \n",
    "        self.data = self.data.drop(columns=[\n",
    "            'world_idx', 'timestep', 'actual_time', 'optimal_time', 'success'\n",
    "        ])\n",
    "\n",
    "        # get all the column values that contain the word lidar\n",
    "        self.lidar_cols = [\"lidar_\" + str(i) for i in range(0, 720, 1)]\n",
    "        # get actions columns\n",
    "        self.actions_cols = ['cmd_vel_linear', 'cmd_vel_angular']\n",
    "        # get other columns\n",
    "        self.non_lidar_cols = ['local_x', 'local_y', 'twist_linear', 'twist_angular']\n",
    "\n",
    "        if mode == \"train\":\n",
    "            # Manually compute the min and max values for each column\n",
    "            self.min = self.data.min()\n",
    "            self.max = self.data.max()\n",
    "\n",
    "            # Save the mean and std to a JSON file\n",
    "            scaler_params = {\n",
    "                'min': self.min.to_dict(),\n",
    "                'max': self.max.to_dict()\n",
    "            }\n",
    "            with open('scaler_params.json', 'w') as f:\n",
    "                json.dump(scaler_params, f)\n",
    "        else:\n",
    "            # Load the mean and std from the JSON file\n",
    "            with open('scaler_params.json', 'r') as f:\n",
    "                scaler_params = json.load(f)\n",
    "            self.min = pd.Series(scaler_params['min'])\n",
    "            self.max = pd.Series(scaler_params['max'])\n",
    "        \n",
    "        # dont normalizer local_x and local_y\n",
    "        self.normalized_data = (self.data - self.min) / (self.max - self.min)\n",
    "        \n",
    "        self.lidar_data = self.normalized_data[self.lidar_cols].values\n",
    "        self.non_lidar_data = self.normalized_data[self.non_lidar_cols].values\n",
    "        self.actions_data = self.normalized_data[self.actions_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lidar = self.lidar_data[idx]\n",
    "        non_lidar = self.non_lidar_data[idx]\n",
    "        actions = self.actions_data[idx]\n",
    "        return lidar, non_lidar, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "import random\n",
    "# set random seed\n",
    "random.seed(42)\n",
    "\n",
    "NO_WORLDS = 300\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "world_ids = [i for i in range(NO_WORLDS)]\n",
    "test_ids = [id for id in range(0, NO_WORLDS, 5)]\n",
    "train_evals = [id for id in world_ids if id not in test_ids]\n",
    "train_ids = random.sample(train_evals, int(NO_WORLDS * TRAIN_RATIO))\n",
    "val_ids = [id for id in train_evals if id not in train_ids]\n",
    "\n",
    "train_df = df[df['world_idx'].isin(train_ids)]\n",
    "val_df = df[df['world_idx'].isin(val_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "30\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ids))\n",
    "print(len(val_ids))\n",
    "print(len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 27, 37, 56, 57, 83, 98, 106, 107, 132, 142, 144, 149, 153, 154, 158, 166, 176, 187, 194, 221, 238, 242, 247, 253, 258, 262, 278, 286, 289]\n"
     ]
    }
   ],
   "source": [
    "print(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140392\n",
      "19838\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KULBarnDataset(train_df, \"train\")\n",
    "val_dataset = KULBarnDataset(val_df, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Length: 140392\n",
      "Val Dataset Length: 19838\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset Length:\", len(train_dataset))\n",
    "print(\"Val Dataset Length:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non lidar shape: torch.Size([64, 4])\n",
      "Lidar shape: torch.Size([64, 720])\n",
      "Train loader size: 2194\n",
      "Val loader size: 310\n",
      "tensor([[0.0572, 0.0575, 0.0584,  ..., 0.1002, 0.0980, 0.0842],\n",
      "        [0.2878, 0.2861, 0.2856,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.2623, 0.2621, 0.2620,  ..., 0.2854, 0.2876, 0.2951],\n",
      "        ...,\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 0.1972, 0.1959, 0.1948],\n",
      "        [0.3197, 0.3185, 0.3181,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.4090, 0.4075, 0.4071,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "       dtype=torch.float64) tensor([[0.9910, 0.5945, 0.9920, 0.4639],\n",
      "        [0.9971, 0.5536, 0.9913, 0.4979],\n",
      "        [0.9983, 0.5412, 0.7446, 0.5244],\n",
      "        [0.9984, 0.4606, 0.9924, 0.6320],\n",
      "        [0.9991, 0.5302, 0.6822, 0.5620],\n",
      "        [0.9995, 0.4781, 0.9925, 0.5063],\n",
      "        [0.9988, 0.5352, 0.9950, 0.5208],\n",
      "        [0.9979, 0.5462, 0.9948, 0.4022],\n",
      "        [0.9949, 0.4285, 0.9940, 0.5229],\n",
      "        [0.9994, 0.5247, 0.9920, 0.5205],\n",
      "        [0.9994, 0.5235, 0.9709, 0.5218],\n",
      "        [0.9999, 0.5107, 0.9912, 0.5222],\n",
      "        [0.9999, 0.4927, 0.9915, 0.5344],\n",
      "        [0.9914, 0.5923, 0.7525, 0.5490],\n",
      "        [0.9642, 0.3141, 0.9937, 0.3351],\n",
      "        [0.9955, 0.5666, 0.9918, 0.5458],\n",
      "        [0.9995, 0.5213, 0.9924, 0.5211],\n",
      "        [0.9534, 0.2892, 0.4853, 0.4467],\n",
      "        [0.9997, 0.5165, 0.9966, 0.5167],\n",
      "        [0.9995, 0.4770, 0.9920, 0.5262],\n",
      "        [0.9997, 0.5173, 0.9917, 0.5433],\n",
      "        [1.0000, 0.4955, 0.9960, 0.5195],\n",
      "        [0.5504, 0.0025, 0.4522, 0.4136],\n",
      "        [0.9981, 0.4568, 0.9919, 0.3846],\n",
      "        [0.9989, 0.4667, 0.9783, 0.5297],\n",
      "        [0.4640, 0.0013, 0.3133, 0.3761],\n",
      "        [0.9557, 0.7057, 0.7944, 0.5831],\n",
      "        [0.9851, 0.6212, 0.9916, 0.4970],\n",
      "        [1.0000, 0.5006, 0.4445, 0.5237],\n",
      "        [0.9941, 0.5768, 0.4228, 0.5222],\n",
      "        [0.9998, 0.4853, 0.9921, 0.5246],\n",
      "        [0.9999, 0.5102, 0.9913, 0.5278],\n",
      "        [0.9986, 0.4624, 0.9927, 0.5184],\n",
      "        [0.9999, 0.5082, 0.9916, 0.5370],\n",
      "        [0.9938, 0.5785, 0.9916, 0.5014],\n",
      "        [0.9782, 0.6461, 0.9929, 0.6885],\n",
      "        [0.9990, 0.4690, 0.9930, 0.5257],\n",
      "        [0.9621, 0.6910, 0.9927, 0.5054],\n",
      "        [0.9998, 0.5150, 0.9922, 0.5613],\n",
      "        [0.9972, 0.5530, 0.9912, 0.4858],\n",
      "        [0.9988, 0.5348, 0.9972, 0.5457],\n",
      "        [0.9989, 0.5337, 0.9916, 0.5387],\n",
      "        [0.9998, 0.4872, 0.9933, 0.5695],\n",
      "        [0.9955, 0.5669, 0.9917, 0.5091],\n",
      "        [0.9957, 0.4343, 0.9908, 0.4773],\n",
      "        [0.9924, 0.5867, 0.9912, 0.5854],\n",
      "        [1.0000, 0.5003, 0.3780, 0.5226],\n",
      "        [0.9973, 0.5522, 0.9914, 0.5065],\n",
      "        [0.9979, 0.5463, 0.9947, 0.3585],\n",
      "        [0.9991, 0.4695, 0.9909, 0.5223],\n",
      "        [0.9933, 0.4184, 0.9919, 0.6116],\n",
      "        [0.9990, 0.5319, 0.9920, 0.5544],\n",
      "        [0.9946, 0.5733, 0.3744, 0.5241],\n",
      "        [0.9996, 0.5188, 0.9934, 0.5242],\n",
      "        [0.9892, 0.6035, 0.9917, 0.5209],\n",
      "        [0.9993, 0.4733, 0.9911, 0.5255],\n",
      "        [1.0000, 0.4967, 0.9920, 0.5225],\n",
      "        [0.9981, 0.5440, 0.9951, 0.5077],\n",
      "        [0.9989, 0.4664, 0.9913, 0.4800],\n",
      "        [0.9762, 0.6524, 0.9929, 0.4429],\n",
      "        [0.9996, 0.5202, 0.9918, 0.4836],\n",
      "        [0.9973, 0.4478, 0.9915, 0.5202],\n",
      "        [0.9920, 0.5892, 0.9912, 0.5013],\n",
      "        [0.9936, 0.5800, 0.9908, 0.5186]], dtype=torch.float64) tensor([[1.0000, 0.4192],\n",
      "        [1.0000, 0.4759],\n",
      "        [0.7582, 0.5126],\n",
      "        [1.0000, 0.6241],\n",
      "        [0.6927, 0.5416],\n",
      "        [1.0000, 0.4926],\n",
      "        [1.0000, 0.4939],\n",
      "        [1.0000, 0.3220],\n",
      "        [1.0000, 0.5446],\n",
      "        [1.0000, 0.5005],\n",
      "        [0.9873, 0.5029],\n",
      "        [1.0000, 0.4952],\n",
      "        [1.0000, 0.5175],\n",
      "        [0.7636, 0.5822],\n",
      "        [1.0000, 0.2753],\n",
      "        [1.0000, 0.5284],\n",
      "        [1.0000, 0.5023],\n",
      "        [0.4964, 0.4386],\n",
      "        [1.0000, 0.4972],\n",
      "        [1.0000, 0.5239],\n",
      "        [1.0000, 0.5245],\n",
      "        [1.0000, 0.4932],\n",
      "        [0.4545, 0.1937],\n",
      "        [1.0000, 0.3507],\n",
      "        [0.9873, 0.5243],\n",
      "        [0.3182, 0.3432],\n",
      "        [0.8073, 0.6369],\n",
      "        [1.0000, 0.3889],\n",
      "        [0.4527, 0.5000],\n",
      "        [0.4309, 0.4987],\n",
      "        [1.0000, 0.5000],\n",
      "        [1.0000, 0.5178],\n",
      "        [1.0000, 0.4899],\n",
      "        [1.0000, 0.5314],\n",
      "        [1.0000, 0.4628],\n",
      "        [1.0000, 0.7039],\n",
      "        [1.0000, 0.5117],\n",
      "        [1.0000, 0.4771],\n",
      "        [1.0000, 0.5673],\n",
      "        [1.0000, 0.4613],\n",
      "        [1.0000, 0.5545],\n",
      "        [1.0000, 0.5140],\n",
      "        [1.0000, 0.5615],\n",
      "        [1.0000, 0.4850],\n",
      "        [1.0000, 0.4455],\n",
      "        [1.0000, 0.5660],\n",
      "        [0.3873, 0.5000],\n",
      "        [1.0000, 0.4823],\n",
      "        [1.0000, 0.3260],\n",
      "        [1.0000, 0.5015],\n",
      "        [1.0000, 0.6153],\n",
      "        [1.0000, 0.5279],\n",
      "        [0.3873, 0.5044],\n",
      "        [1.0000, 0.5083],\n",
      "        [1.0000, 0.4958],\n",
      "        [1.0000, 0.5043],\n",
      "        [1.0000, 0.5026],\n",
      "        [1.0000, 0.4804],\n",
      "        [1.0000, 0.4302],\n",
      "        [1.0000, 0.4053],\n",
      "        [1.0000, 0.4637],\n",
      "        [1.0000, 0.5248],\n",
      "        [1.0000, 0.4726],\n",
      "        [1.0000, 0.4713]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "# test dataloader\n",
    "lidar, non_lidar, actions = next(iter(train_loader))\n",
    "print(f\"Non lidar shape: {non_lidar.shape}\")\n",
    "print(f\"Lidar shape: {lidar.shape}\")\n",
    "# print size dataloader\n",
    "print(f\"Train loader size: {len(train_loader)}\")\n",
    "print(f\"Val loader size: {len(val_loader)}\")\n",
    "print(lidar, non_lidar, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0572, 0.0575, 0.0584,  ..., 0.1002, 0.0980, 0.0842],\n",
       "        [0.2878, 0.2861, 0.2856,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.2623, 0.2621, 0.2620,  ..., 0.2854, 0.2876, 0.2951],\n",
       "        ...,\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 0.1972, 0.1959, 0.1948],\n",
       "        [0.3197, 0.3185, 0.3181,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.4090, 0.4075, 0.4071,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9910, 0.5945, 0.9920, 0.4639],\n",
       "        [0.9971, 0.5536, 0.9913, 0.4979],\n",
       "        [0.9983, 0.5412, 0.7446, 0.5244],\n",
       "        [0.9984, 0.4606, 0.9924, 0.6320],\n",
       "        [0.9991, 0.5302, 0.6822, 0.5620],\n",
       "        [0.9995, 0.4781, 0.9925, 0.5063],\n",
       "        [0.9988, 0.5352, 0.9950, 0.5208],\n",
       "        [0.9979, 0.5462, 0.9948, 0.4022],\n",
       "        [0.9949, 0.4285, 0.9940, 0.5229],\n",
       "        [0.9994, 0.5247, 0.9920, 0.5205],\n",
       "        [0.9994, 0.5235, 0.9709, 0.5218],\n",
       "        [0.9999, 0.5107, 0.9912, 0.5222],\n",
       "        [0.9999, 0.4927, 0.9915, 0.5344],\n",
       "        [0.9914, 0.5923, 0.7525, 0.5490],\n",
       "        [0.9642, 0.3141, 0.9937, 0.3351],\n",
       "        [0.9955, 0.5666, 0.9918, 0.5458],\n",
       "        [0.9995, 0.5213, 0.9924, 0.5211],\n",
       "        [0.9534, 0.2892, 0.4853, 0.4467],\n",
       "        [0.9997, 0.5165, 0.9966, 0.5167],\n",
       "        [0.9995, 0.4770, 0.9920, 0.5262],\n",
       "        [0.9997, 0.5173, 0.9917, 0.5433],\n",
       "        [1.0000, 0.4955, 0.9960, 0.5195],\n",
       "        [0.5504, 0.0025, 0.4522, 0.4136],\n",
       "        [0.9981, 0.4568, 0.9919, 0.3846],\n",
       "        [0.9989, 0.4667, 0.9783, 0.5297],\n",
       "        [0.4640, 0.0013, 0.3133, 0.3761],\n",
       "        [0.9557, 0.7057, 0.7944, 0.5831],\n",
       "        [0.9851, 0.6212, 0.9916, 0.4970],\n",
       "        [1.0000, 0.5006, 0.4445, 0.5237],\n",
       "        [0.9941, 0.5768, 0.4228, 0.5222],\n",
       "        [0.9998, 0.4853, 0.9921, 0.5246],\n",
       "        [0.9999, 0.5102, 0.9913, 0.5278],\n",
       "        [0.9986, 0.4624, 0.9927, 0.5184],\n",
       "        [0.9999, 0.5082, 0.9916, 0.5370],\n",
       "        [0.9938, 0.5785, 0.9916, 0.5014],\n",
       "        [0.9782, 0.6461, 0.9929, 0.6885],\n",
       "        [0.9990, 0.4690, 0.9930, 0.5257],\n",
       "        [0.9621, 0.6910, 0.9927, 0.5054],\n",
       "        [0.9998, 0.5150, 0.9922, 0.5613],\n",
       "        [0.9972, 0.5530, 0.9912, 0.4858],\n",
       "        [0.9988, 0.5348, 0.9972, 0.5457],\n",
       "        [0.9989, 0.5337, 0.9916, 0.5387],\n",
       "        [0.9998, 0.4872, 0.9933, 0.5695],\n",
       "        [0.9955, 0.5669, 0.9917, 0.5091],\n",
       "        [0.9957, 0.4343, 0.9908, 0.4773],\n",
       "        [0.9924, 0.5867, 0.9912, 0.5854],\n",
       "        [1.0000, 0.5003, 0.3780, 0.5226],\n",
       "        [0.9973, 0.5522, 0.9914, 0.5065],\n",
       "        [0.9979, 0.5463, 0.9947, 0.3585],\n",
       "        [0.9991, 0.4695, 0.9909, 0.5223],\n",
       "        [0.9933, 0.4184, 0.9919, 0.6116],\n",
       "        [0.9990, 0.5319, 0.9920, 0.5544],\n",
       "        [0.9946, 0.5733, 0.3744, 0.5241],\n",
       "        [0.9996, 0.5188, 0.9934, 0.5242],\n",
       "        [0.9892, 0.6035, 0.9917, 0.5209],\n",
       "        [0.9993, 0.4733, 0.9911, 0.5255],\n",
       "        [1.0000, 0.4967, 0.9920, 0.5225],\n",
       "        [0.9981, 0.5440, 0.9951, 0.5077],\n",
       "        [0.9989, 0.4664, 0.9913, 0.4800],\n",
       "        [0.9762, 0.6524, 0.9929, 0.4429],\n",
       "        [0.9996, 0.5202, 0.9918, 0.4836],\n",
       "        [0.9973, 0.4478, 0.9915, 0.5202],\n",
       "        [0.9920, 0.5892, 0.9912, 0.5013],\n",
       "        [0.9936, 0.5800, 0.9908, 0.5186]], dtype=torch.float64)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.4192],\n",
       "        [1.0000, 0.4759],\n",
       "        [0.7582, 0.5126],\n",
       "        [1.0000, 0.6241],\n",
       "        [0.6927, 0.5416],\n",
       "        [1.0000, 0.4926],\n",
       "        [1.0000, 0.4939],\n",
       "        [1.0000, 0.3220],\n",
       "        [1.0000, 0.5446],\n",
       "        [1.0000, 0.5005],\n",
       "        [0.9873, 0.5029],\n",
       "        [1.0000, 0.4952],\n",
       "        [1.0000, 0.5175],\n",
       "        [0.7636, 0.5822],\n",
       "        [1.0000, 0.2753],\n",
       "        [1.0000, 0.5284],\n",
       "        [1.0000, 0.5023],\n",
       "        [0.4964, 0.4386],\n",
       "        [1.0000, 0.4972],\n",
       "        [1.0000, 0.5239],\n",
       "        [1.0000, 0.5245],\n",
       "        [1.0000, 0.4932],\n",
       "        [0.4545, 0.1937],\n",
       "        [1.0000, 0.3507],\n",
       "        [0.9873, 0.5243],\n",
       "        [0.3182, 0.3432],\n",
       "        [0.8073, 0.6369],\n",
       "        [1.0000, 0.3889],\n",
       "        [0.4527, 0.5000],\n",
       "        [0.4309, 0.4987],\n",
       "        [1.0000, 0.5000],\n",
       "        [1.0000, 0.5178],\n",
       "        [1.0000, 0.4899],\n",
       "        [1.0000, 0.5314],\n",
       "        [1.0000, 0.4628],\n",
       "        [1.0000, 0.7039],\n",
       "        [1.0000, 0.5117],\n",
       "        [1.0000, 0.4771],\n",
       "        [1.0000, 0.5673],\n",
       "        [1.0000, 0.4613],\n",
       "        [1.0000, 0.5545],\n",
       "        [1.0000, 0.5140],\n",
       "        [1.0000, 0.5615],\n",
       "        [1.0000, 0.4850],\n",
       "        [1.0000, 0.4455],\n",
       "        [1.0000, 0.5660],\n",
       "        [0.3873, 0.5000],\n",
       "        [1.0000, 0.4823],\n",
       "        [1.0000, 0.3260],\n",
       "        [1.0000, 0.5015],\n",
       "        [1.0000, 0.6153],\n",
       "        [1.0000, 0.5279],\n",
       "        [0.3873, 0.5044],\n",
       "        [1.0000, 0.5083],\n",
       "        [1.0000, 0.4958],\n",
       "        [1.0000, 0.5043],\n",
       "        [1.0000, 0.5026],\n",
       "        [1.0000, 0.4804],\n",
       "        [1.0000, 0.4302],\n",
       "        [1.0000, 0.4053],\n",
       "        [1.0000, 0.4637],\n",
       "        [1.0000, 0.5248],\n",
       "        [1.0000, 0.4726],\n",
       "        [1.0000, 0.4713]], dtype=torch.float64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_lidar_features, num_non_lidar_features, num_actions, nframes=1):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.act_fea_cv1 = nn.Conv1d(\n",
    "            in_channels=nframes, out_channels=32, kernel_size=5, stride=2, padding=6, padding_mode='circular'\n",
    "        )\n",
    "        self.act_fea_cv2 = nn.Conv1d(\n",
    "            in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1\n",
    "        )\n",
    "\n",
    "        conv_output_size = (num_lidar_features - 5 + 2*6) // 2 + 1  # Output size after self.act_fea_cv1\n",
    "        conv_output_size = (conv_output_size - 3 + 2*1) // 2 + 1  # Output size after self.act_fea_cv2\n",
    "        conv_output_size *= 32  # Multiply by the number of output channels\n",
    "\n",
    "        # Calculate the output size of the CNN\n",
    "        self.fc1 = nn.Linear(conv_output_size, 64)\n",
    "        self.fc2 = nn.Linear(64 + num_non_lidar_features, 32)\n",
    "        self.fc3 = nn.Linear(32, num_actions)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, lidar, non_lidar):\n",
    "        lidar = lidar.unsqueeze(1)  # Add channel dimension\n",
    "        feat = F.relu(self.act_fea_cv1(lidar))\n",
    "        feat = F.relu(self.act_fea_cv2(feat))\n",
    "        feat = feat.view(feat.shape[0], -1)\n",
    "        feat = F.relu(self.fc1(feat))\n",
    "        feat = torch.cat((feat, non_lidar), dim=-1)\n",
    "        feat = F.relu(self.fc2(feat))\n",
    "        feat = self.fc3(feat)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_lidar_features = len(train_dataset.lidar_cols)\n",
    "num_non_lidar_features = len(train_dataset.non_lidar_cols)\n",
    "num_actions = len(train_dataset.actions_cols)\n",
    "model = CNNModel(num_lidar_features, num_non_lidar_features, num_actions)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Move the model and loss function to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for lidar, non_lidar, actions in tqdm(train_loader):\n",
    "\n",
    "        # Move the data to the device that is used\n",
    "        lidar = lidar.to(device)\n",
    "        non_lidar = non_lidar.to(device)\n",
    "        actions = actions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        actions_pred = model(lidar.float(), non_lidar.float())\n",
    "        loss = loss_fn(actions_pred, actions.float())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # return the average loss for this epoch\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    for lidar, non_lidar, actions in tqdm(test_loader):\n",
    "        # Move the data to the device that is used\n",
    "        lidar = lidar.to(device)\n",
    "        non_lidar = non_lidar.to(device)\n",
    "        actions = actions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        actions_pred = model(lidar.float(), non_lidar.float())\n",
    "        loss = loss_fn(actions_pred, actions.float())\n",
    "\n",
    "        # Save the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # return the average loss for this epoch\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:01<00:00, 206.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random val loss: 0.11803823808929131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "NUM_EPOCHS = 0\n",
    "\n",
    "random_val_loss = test_model(model, val_loader, loss_fn)\n",
    "print(\"Random val loss:\", random_val_loss)\n",
    "sys.stdout.flush()\n",
    "\n",
    "cnn_train_losses = []\n",
    "cnn_val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_model(model, train_loader, loss_fn, optimizer)\n",
    "    val_loss = test_model(model, val_loader, loss_fn)\n",
    "    cnn_train_losses.append(train_loss)\n",
    "    cnn_val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss} | Val Loss: {val_loss}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping due to no improvement after {} epochs.\".format(patience))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cnn_train_losses, label='Train Loss')\n",
    "plt.plot(cnn_val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'cnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load file and check MSELoss\n",
    "# model = CNNModel(num_lidar_features, num_non_lidar_features, num_actions)\n",
    "# model.load_state_dict(torch.load('cnn_model.pth', map_location=torch.device('cpu')))\n",
    "# model.eval()\n",
    "# device = 'cpu'\n",
    "\n",
    "# # take world idx 0 as example\n",
    "# dataset = KULBarnDataset(df[df['world_idx'] == 0][df['timestep']>0], \"val\")\n",
    "# loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# final_val_loss = test_model(model, loader, loss_fn)\n",
    "# print(\"Final val loss:\", final_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_lidar_features, num_non_lidar_features, num_actions, d_model=64, nhead=4, num_encoder_layers=3, num_patches=36):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_patches = num_patches  # Number of patches\n",
    "        self.patch_size = num_lidar_features // self.num_patches\n",
    "\n",
    "        # Positional Encoding for the Encoder\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(self.num_patches, d_model))\n",
    "\n",
    "        # Input Embedding for Encoder (LiDAR data)\n",
    "        self.lidar_embedding = nn.Linear(self.patch_size, d_model)\n",
    "\n",
    "        # Transformer Encoder for LiDAR data (first encoder)\n",
    "        encoder_layer_1 = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder_1 = nn.TransformerEncoder(encoder_layer_1, num_layers=num_encoder_layers)\n",
    "\n",
    "        # Input Embedding for Non-LiDAR data (values)\n",
    "        self.non_lidar_embedding = nn.Linear(num_non_lidar_features, d_model)\n",
    "\n",
    "        # Transformer Decoder (cross-attention using Q and K from LiDAR encoder and V from non-lidar data)\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead)\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Linear(d_model, d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Second Encoder Layer for post-processing\n",
    "        encoder_layer_2 = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder_2 = nn.TransformerEncoder(encoder_layer_2, num_layers=num_encoder_layers-1)\n",
    "\n",
    "        # Linear layer to map the transformer output to actions\n",
    "        self.fc_out = nn.Linear(d_model * num_patches, num_actions)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, lidar, non_lidar):\n",
    "        batch_size = lidar.size(0)\n",
    "        \n",
    "        # Reshape LiDAR input into patches\n",
    "        lidar_patches = lidar.view(batch_size, self.num_patches, self.patch_size)\n",
    "\n",
    "        # Linear projection of LiDAR patches and adding positional encoding\n",
    "        lidar_embed = self.lidar_embedding(lidar_patches) + self.positional_encoding.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        lidar_embed = lidar_embed.permute(1, 0, 2)  # Convert to (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Process through the transformer encoder for LiDAR data\n",
    "        lidar_encoded = self.transformer_encoder_1(lidar_embed)  # Shape: (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Process non-lidar data through input embedding\n",
    "        non_lidar_embed = self.non_lidar_embedding(non_lidar).unsqueeze(0)  # Shape: (1, batch_size, d_model)\n",
    "\n",
    "        # Repeat the non-lidar embeddings along the sequence length to match the LiDAR sequence length\n",
    "        non_lidar_embed = non_lidar_embed.repeat(self.num_patches, 1, 1)  # Shape: (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Cross-attention: Use LiDAR encoded data as Q and K, non-lidar as V\n",
    "        non_lidar_attended, _ = self.multihead_attention(query=lidar_encoded, key=lidar_encoded, value=non_lidar_embed)\n",
    "        non_lidar_embedding = self.norm_1(non_lidar_attended + non_lidar_embed)\n",
    "        non_lidar_ff = self.ff(non_lidar_attended)\n",
    "        non_lidar_embedding = self.norm_2(non_lidar_ff + non_lidar_attended)\n",
    "\n",
    "        # Process the output of the cross-attention through the second encoder layer\n",
    "        encoder_output = self.transformer_encoder_2(non_lidar_embedding)  # (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Concatenate the encoder output over the sequence length\n",
    "        encoder_output = encoder_output.permute(1, 0, 2)\n",
    "        encoder_output = encoder_output.reshape(batch_size, -1)\n",
    "\n",
    "        # Final linear layer to get the predicted actions\n",
    "        actions = self.fc_out(encoder_output)\n",
    "\n",
    "        actions = self.sigmoid(actions)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomMSELoss(nn.Module):\n",
    "#     def __init__(self, weight):\n",
    "#         super(CustomMSELoss, self).__init__()\n",
    "#         self.weight = weight\n",
    "\n",
    "#     def forward(self, output, target):\n",
    "#         # Assume output and target are two-element tuples\n",
    "#         loss1 = (output[:,0] - target[:,0]) ** 2\n",
    "#         loss2 = self.weight * (output[:,1] - target[:,1]) ** 4\n",
    "#         total_loss = loss1 + loss2\n",
    "#         return total_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_lidar_features = len(train_dataset.lidar_cols)\n",
    "num_non_lidar_features = len(train_dataset.non_lidar_cols)\n",
    "num_actions = len(train_dataset.actions_cols)\n",
    "model = TransformerModel(num_lidar_features, num_non_lidar_features, num_actions)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "# Move the model and loss function to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:02<00:00, 152.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random val loss: 0.14672352211369621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2194/2194 [00:32<00:00, 66.65it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 222.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.022565175893987335 | Val Loss: 0.006304118891186532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:29<00:00, 73.94it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 230.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Loss: 0.0033379217094861467 | Val Loss: 0.0013619818781960696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:31<00:00, 70.22it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 210.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Loss: 0.0019285483958860546 | Val Loss: 0.000952554183905088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:32<00:00, 67.17it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 209.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Loss: 0.0014891498964889028 | Val Loss: 0.0007797201114778047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:33<00:00, 65.24it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 200.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Loss: 0.0012422839529756516 | Val Loss: 0.000686960464360797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:45<00:00, 48.26it/s]\n",
      "100%|██████████| 310/310 [00:03<00:00, 96.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Loss: 0.0011009623822735948 | Val Loss: 0.0006510963050870135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:56<00:00, 38.55it/s]\n",
      "100%|██████████| 310/310 [00:03<00:00, 84.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Loss: 0.0010199386138269896 | Val Loss: 0.000623969144174724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:59<00:00, 37.17it/s]\n",
      "100%|██████████| 310/310 [00:03<00:00, 82.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Loss: 0.000959980379889928 | Val Loss: 0.000605692127224287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:37<00:00, 58.50it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 189.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train Loss: 0.0009133712093000077 | Val Loss: 0.0005880626969971467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:34<00:00, 64.01it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 176.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train Loss: 0.0008779871298868078 | Val Loss: 0.000575486867508575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:34<00:00, 63.14it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 197.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train Loss: 0.0008503863842242695 | Val Loss: 0.0005732788723171032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:35<00:00, 62.16it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 180.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train Loss: 0.0008324217713267013 | Val Loss: 0.0005600574206739346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:45<00:00, 48.35it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 154.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train Loss: 0.0008138336873848994 | Val Loss: 0.0005564762062447464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:40<00:00, 53.74it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 135.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train Loss: 0.000793347891282221 | Val Loss: 0.0005482638171425576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:43<00:00, 50.24it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 156.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train Loss: 0.0007808991060745579 | Val Loss: 0.0005538729945798086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:42<00:00, 51.50it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 155.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train Loss: 0.0007684485373021856 | Val Loss: 0.0005588311778527925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:42<00:00, 52.09it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 149.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train Loss: 0.0007573100525254716 | Val Loss: 0.000541816550738135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:41<00:00, 52.78it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 154.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Train Loss: 0.0007496677209502604 | Val Loss: 0.0005416472970248452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:45<00:00, 47.86it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 142.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Train Loss: 0.0007374043926461231 | Val Loss: 0.0005331481486116959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:43<00:00, 50.41it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 203.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Train Loss: 0.000731849411303821 | Val Loss: 0.0005322198183655987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "random_val_loss = test_model(model, val_loader, loss_fn)\n",
    "print(\"Random val loss:\", random_val_loss)\n",
    "\n",
    "transformer_train_losses = []\n",
    "transformer_val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_model(model, train_loader, loss_fn, optimizer)\n",
    "    val_loss = test_model(model, val_loader, loss_fn)\n",
    "    transformer_train_losses.append(train_loss)\n",
    "    transformer_val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss} | Val Loss: {val_loss}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping due to no improvement after {} epochs.\".format(patience))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(transformer_train_losses, label='Transformer Train Loss')  \n",
    "plt.plot(transformer_val_losses, label='Transformer Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    for lidar, non_lidar, actions in tqdm(test_loader):\n",
    "        # Move the data to the device that is used\n",
    "        lidar = lidar.to(device)\n",
    "        non_lidar = non_lidar.to(device)\n",
    "        actions = actions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        actions_pred = model(lidar.float(), non_lidar.float())        \n",
    "        loss = loss_fn(actions_pred, actions.float())\n",
    "\n",
    "        print(non_lidar)\n",
    "        print(actions_pred)\n",
    "        print(actions)\n",
    "        print(loss)\n",
    "\n",
    "        # Save the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # return the average loss for this epoch\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/665 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 9/665 [00:00<00:07, 86.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9949, 0.5713, 0.3246, 0.5223]], dtype=torch.float64)\n",
      "tensor([[0.3382, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3327, 0.5021]], dtype=torch.float64)\n",
      "tensor(3.0956e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9949, 0.5713, 0.3246, 0.5223]], dtype=torch.float64)\n",
      "tensor([[0.3382, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3327, 0.5023]], dtype=torch.float64)\n",
      "tensor(2.9787e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9949, 0.5713, 0.3246, 0.5223]], dtype=torch.float64)\n",
      "tensor([[0.3382, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3327, 0.5023]], dtype=torch.float64)\n",
      "tensor(2.9787e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9949, 0.5715, 0.3300, 0.5241]], dtype=torch.float64)\n",
      "tensor([[0.3433, 0.5091]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3382, 0.5025]], dtype=torch.float64)\n",
      "tensor(3.4320e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9948, 0.5719, 0.3355, 0.5220]], dtype=torch.float64)\n",
      "tensor([[0.3485, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3436, 0.5028]], dtype=torch.float64)\n",
      "tensor(2.2546e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9948, 0.5721, 0.3409, 0.5231]], dtype=torch.float64)\n",
      "tensor([[0.3537, 0.5082]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3491, 0.5030]], dtype=torch.float64)\n",
      "tensor(2.4465e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9947, 0.5724, 0.3463, 0.5235]], dtype=torch.float64)\n",
      "tensor([[0.3590, 0.5085]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3545, 0.5032]], dtype=torch.float64)\n",
      "tensor(2.3912e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9947, 0.5728, 0.3518, 0.5220]], dtype=torch.float64)\n",
      "tensor([[0.3643, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3600, 0.5034]], dtype=torch.float64)\n",
      "tensor(1.6596e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9946, 0.5732, 0.3572, 0.5235]], dtype=torch.float64)\n",
      "tensor([[0.3697, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3655, 0.5036]], dtype=torch.float64)\n",
      "tensor(2.0421e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9946, 0.5736, 0.3627, 0.5221]], dtype=torch.float64)\n",
      "tensor([[0.3751, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3709, 0.5038]], dtype=torch.float64)\n",
      "tensor(1.4676e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9945, 0.5740, 0.3681, 0.5235]], dtype=torch.float64)\n",
      "tensor([[0.3806, 0.5083]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3764, 0.5040]], dtype=torch.float64)\n",
      "tensor(1.8276e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9944, 0.5745, 0.3736, 0.5232]], dtype=torch.float64)\n",
      "tensor([[0.3861, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3818, 0.5042]], dtype=torch.float64)\n",
      "tensor(1.6718e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9943, 0.5750, 0.3790, 0.5225]], dtype=torch.float64)\n",
      "tensor([[0.3917, 0.5075]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3873, 0.5045]], dtype=torch.float64)\n",
      "tensor(1.4153e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9943, 0.5755, 0.3843, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.3971, 0.5079]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3927, 0.5047]], dtype=torch.float64)\n",
      "tensor(1.4708e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9950, 0.5707, 0.3896, 0.5224]], dtype=torch.float64)\n",
      "tensor([[0.4026, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.3982, 0.5049]], dtype=torch.float64)\n",
      "tensor(1.1406e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9949, 0.5712, 0.3951, 0.5218]], dtype=torch.float64)\n",
      "tensor([[0.4084, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4036, 0.5051]], dtype=torch.float64)\n",
      "tensor(1.1754e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 26/665 [00:00<00:09, 70.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9948, 0.5717, 0.4005, 0.5227]], dtype=torch.float64)\n",
      "tensor([[0.4141, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4091, 0.5053]], dtype=torch.float64)\n",
      "tensor(1.4002e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9948, 0.5722, 0.4059, 0.5231]], dtype=torch.float64)\n",
      "tensor([[0.4199, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4145, 0.5055]], dtype=torch.float64)\n",
      "tensor(1.6179e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9947, 0.5728, 0.4114, 0.5238]], dtype=torch.float64)\n",
      "tensor([[0.4259, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4200, 0.5058]], dtype=torch.float64)\n",
      "tensor(1.9838e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9946, 0.5734, 0.4169, 0.5240]], dtype=torch.float64)\n",
      "tensor([[0.4318, 0.5083]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4255, 0.5060]], dtype=torch.float64)\n",
      "tensor(2.2811e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9945, 0.5740, 0.4223, 0.5247]], dtype=torch.float64)\n",
      "tensor([[0.4377, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4309, 0.5066]], dtype=torch.float64)\n",
      "tensor(2.5870e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9944, 0.5747, 0.4279, 0.5242]], dtype=torch.float64)\n",
      "tensor([[0.4437, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4364, 0.5068]], dtype=torch.float64)\n",
      "tensor(2.8300e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9951, 0.5699, 0.4333, 0.5243]], dtype=torch.float64)\n",
      "tensor([[0.4496, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4418, 0.5065]], dtype=torch.float64)\n",
      "tensor(3.1301e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9950, 0.5704, 0.4388, 0.5256]], dtype=torch.float64)\n",
      "tensor([[0.4555, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4473, 0.5067]], dtype=torch.float64)\n",
      "tensor(3.7498e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9949, 0.5710, 0.4442, 0.5265]], dtype=torch.float64)\n",
      "tensor([[0.4614, 0.5103]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4527, 0.5069]], dtype=torch.float64)\n",
      "tensor(4.3247e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9948, 0.5716, 0.4496, 0.5267]], dtype=torch.float64)\n",
      "tensor([[0.4673, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4582, 0.5071]], dtype=torch.float64)\n",
      "tensor(4.7530e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9948, 0.5722, 0.4551, 0.5274]], dtype=torch.float64)\n",
      "tensor([[0.4731, 0.5115]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4636, 0.5073]], dtype=torch.float64)\n",
      "tensor(5.3805e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9947, 0.5729, 0.4605, 0.5268]], dtype=torch.float64)\n",
      "tensor([[0.4790, 0.5111]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4691, 0.5065]], dtype=torch.float64)\n",
      "tensor(5.9735e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 41/665 [00:00<00:09, 68.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9946, 0.5736, 0.4659, 0.5274]], dtype=torch.float64)\n",
      "tensor([[0.4848, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4745, 0.5067]], dtype=torch.float64)\n",
      "tensor(6.6117e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9952, 0.5689, 0.4713, 0.5278]], dtype=torch.float64)\n",
      "tensor([[0.4904, 0.5116]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4800, 0.5068]], dtype=torch.float64)\n",
      "tensor(6.5717e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9951, 0.5696, 0.4767, 0.5272]], dtype=torch.float64)\n",
      "tensor([[0.4960, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4855, 0.5070]], dtype=torch.float64)\n",
      "tensor(6.4947e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9950, 0.5702, 0.4821, 0.5284]], dtype=torch.float64)\n",
      "tensor([[0.5015, 0.5126]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4909, 0.5072]], dtype=torch.float64)\n",
      "tensor(7.0963e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9949, 0.5710, 0.4874, 0.5292]], dtype=torch.float64)\n",
      "tensor([[0.5069, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.4964, 0.5074]], dtype=torch.float64)\n",
      "tensor(7.4533e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9948, 0.5719, 0.4927, 0.5260]], dtype=torch.float64)\n",
      "tensor([[0.5122, 0.5111]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5018, 0.5075]], dtype=torch.float64)\n",
      "tensor(5.9960e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9955, 0.5673, 0.4981, 0.5261]], dtype=torch.float64)\n",
      "tensor([[0.5176, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5073, 0.5077]], dtype=torch.float64)\n",
      "tensor(5.7443e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9954, 0.5679, 0.5035, 0.5293]], dtype=torch.float64)\n",
      "tensor([[0.5231, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5127, 0.5073]], dtype=torch.float64)\n",
      "tensor(7.3762e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9953, 0.5687, 0.5089, 0.5272]], dtype=torch.float64)\n",
      "tensor([[0.5284, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5182, 0.5075]], dtype=torch.float64)\n",
      "tensor(6.3064e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9951, 0.5696, 0.5143, 0.5268]], dtype=torch.float64)\n",
      "tensor([[0.5338, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5236, 0.5075]], dtype=torch.float64)\n",
      "tensor(6.2114e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9950, 0.5705, 0.5197, 0.5273]], dtype=torch.float64)\n",
      "tensor([[0.5392, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5291, 0.5077]], dtype=torch.float64)\n",
      "tensor(6.4109e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9956, 0.5659, 0.5250, 0.5281]], dtype=torch.float64)\n",
      "tensor([[0.5446, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5345, 0.5078]], dtype=torch.float64)\n",
      "tensor(6.3332e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9955, 0.5668, 0.5304, 0.5272]], dtype=torch.float64)\n",
      "tensor([[0.5499, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5400, 0.5084]], dtype=torch.float64)\n",
      "tensor(5.7040e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9954, 0.5676, 0.5358, 0.5299]], dtype=torch.float64)\n",
      "tensor([[0.5553, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5455, 0.5085]], dtype=torch.float64)\n",
      "tensor(7.1212e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9953, 0.5685, 0.5412, 0.5286]], dtype=torch.float64)\n",
      "tensor([[0.5607, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5509, 0.5076]], dtype=torch.float64)\n",
      "tensor(6.9945e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 56/665 [00:00<00:08, 68.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9959, 0.5642, 0.5466, 0.5279]], dtype=torch.float64)\n",
      "tensor([[0.5660, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5564, 0.5078]], dtype=torch.float64)\n",
      "tensor(6.0865e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9957, 0.5655, 0.5520, 0.5232]], dtype=torch.float64)\n",
      "tensor([[0.5711, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5618, 0.5079]], dtype=torch.float64)\n",
      "tensor(4.3940e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9955, 0.5666, 0.5573, 0.5241]], dtype=torch.float64)\n",
      "tensor([[0.5764, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5673, 0.5075]], dtype=torch.float64)\n",
      "tensor(4.5131e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9954, 0.5680, 0.5627, 0.5212]], dtype=torch.float64)\n",
      "tensor([[0.5816, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5727, 0.5076]], dtype=torch.float64)\n",
      "tensor(3.8950e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9959, 0.5639, 0.5682, 0.5214]], dtype=torch.float64)\n",
      "tensor([[0.5870, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5782, 0.5080]], dtype=torch.float64)\n",
      "tensor(3.9044e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9958, 0.5650, 0.5737, 0.5246]], dtype=torch.float64)\n",
      "tensor([[0.5924, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5836, 0.5081]], dtype=torch.float64)\n",
      "tensor(4.1854e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9956, 0.5663, 0.5791, 0.5235]], dtype=torch.float64)\n",
      "tensor([[0.5977, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5891, 0.5082]], dtype=torch.float64)\n",
      "tensor(3.8234e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9954, 0.5676, 0.5846, 0.5247]], dtype=torch.float64)\n",
      "tensor([[0.6030, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5945, 0.5084]], dtype=torch.float64)\n",
      "tensor(3.9830e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9960, 0.5635, 0.5900, 0.5239]], dtype=torch.float64)\n",
      "tensor([[0.6083, 0.5101]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6000, 0.5085]], dtype=torch.float64)\n",
      "tensor(3.6125e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9958, 0.5647, 0.5954, 0.5239]], dtype=torch.float64)\n",
      "tensor([[0.6135, 0.5104]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6055, 0.5082]], dtype=torch.float64)\n",
      "tensor(3.5072e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9957, 0.5658, 0.6006, 0.5269]], dtype=torch.float64)\n",
      "tensor([[0.6186, 0.5135]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6109, 0.5083]], dtype=torch.float64)\n",
      "tensor(4.3235e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9962, 0.5617, 0.6059, 0.5284]], dtype=torch.float64)\n",
      "tensor([[0.6238, 0.5144]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6164, 0.5084]], dtype=torch.float64)\n",
      "tensor(4.4987e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9960, 0.5628, 0.6112, 0.5272]], dtype=torch.float64)\n",
      "tensor([[0.6287, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6218, 0.5085]], dtype=torch.float64)\n",
      "tensor(3.5924e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 72/665 [00:01<00:08, 73.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9959, 0.5639, 0.6165, 0.5295]], dtype=torch.float64)\n",
      "tensor([[0.6339, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6273, 0.5086]], dtype=torch.float64)\n",
      "tensor(4.8197e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9964, 0.5600, 0.6219, 0.5281]], dtype=torch.float64)\n",
      "tensor([[0.6391, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6327, 0.5088]], dtype=torch.float64)\n",
      "tensor(3.3675e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9963, 0.5611, 0.6274, 0.5282]], dtype=torch.float64)\n",
      "tensor([[0.6443, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6382, 0.5089]], dtype=torch.float64)\n",
      "tensor(3.2986e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9961, 0.5623, 0.6327, 0.5275]], dtype=torch.float64)\n",
      "tensor([[0.6493, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6436, 0.5090]], dtype=torch.float64)\n",
      "tensor(2.7986e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9966, 0.5584, 0.6382, 0.5295]], dtype=torch.float64)\n",
      "tensor([[0.6547, 0.5153]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6491, 0.5082]], dtype=torch.float64)\n",
      "tensor(4.1098e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9964, 0.5596, 0.6437, 0.5269]], dtype=torch.float64)\n",
      "tensor([[0.6598, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6545, 0.5083]], dtype=torch.float64)\n",
      "tensor(2.4569e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9963, 0.5609, 0.6494, 0.5274]], dtype=torch.float64)\n",
      "tensor([[0.6653, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6600, 0.5072]], dtype=torch.float64)\n",
      "tensor(3.5013e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9967, 0.5572, 0.6546, 0.5271]], dtype=torch.float64)\n",
      "tensor([[0.6702, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6655, 0.5073]], dtype=torch.float64)\n",
      "tensor(2.6827e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9966, 0.5586, 0.6599, 0.5245]], dtype=torch.float64)\n",
      "tensor([[0.6751, 0.5104]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6709, 0.5074]], dtype=torch.float64)\n",
      "tensor(1.3492e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9964, 0.5601, 0.6655, 0.5243]], dtype=torch.float64)\n",
      "tensor([[0.6803, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6764, 0.5071]], dtype=torch.float64)\n",
      "tensor(1.3527e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9968, 0.5565, 0.6711, 0.5244]], dtype=torch.float64)\n",
      "tensor([[0.6857, 0.5101]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6818, 0.5072]], dtype=torch.float64)\n",
      "tensor(1.1537e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9966, 0.5578, 0.6766, 0.5260]], dtype=torch.float64)\n",
      "tensor([[0.6910, 0.5119]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6873, 0.5074]], dtype=torch.float64)\n",
      "tensor(1.7108e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9965, 0.5593, 0.6822, 0.5254]], dtype=torch.float64)\n",
      "tensor([[0.6962, 0.5116]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6927, 0.5075]], dtype=torch.float64)\n",
      "tensor(1.4246e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9969, 0.5556, 0.6877, 0.5280]], dtype=torch.float64)\n",
      "tensor([[0.7015, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6982, 0.5076]], dtype=torch.float64)\n",
      "tensor(2.4452e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9967, 0.5570, 0.6930, 0.5269]], dtype=torch.float64)\n",
      "tensor([[0.7064, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7036, 0.5071]], dtype=torch.float64)\n",
      "tensor(1.9958e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9966, 0.5584, 0.6984, 0.5267]], dtype=torch.float64)\n",
      "tensor([[0.7114, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7091, 0.5072]], dtype=torch.float64)\n",
      "tensor(1.8460e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 81/665 [00:01<00:07, 75.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9970, 0.5548, 0.7037, 0.5258]], dtype=torch.float64)\n",
      "tensor([[0.7164, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7145, 0.5078]], dtype=torch.float64)\n",
      "tensor(7.9634e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9968, 0.5561, 0.7091, 0.5263]], dtype=torch.float64)\n",
      "tensor([[0.7214, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7200, 0.5079]], dtype=torch.float64)\n",
      "tensor(9.9319e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9972, 0.5525, 0.7145, 0.5279]], dtype=torch.float64)\n",
      "tensor([[0.7265, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7255, 0.5080]], dtype=torch.float64)\n",
      "tensor(1.4538e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9971, 0.5537, 0.7198, 0.5289]], dtype=torch.float64)\n",
      "tensor([[0.7314, 0.5146]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7309, 0.5072]], dtype=torch.float64)\n",
      "tensor(2.7216e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9970, 0.5551, 0.7252, 0.5277]], dtype=torch.float64)\n",
      "tensor([[0.7363, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7364, 0.5073]], dtype=torch.float64)\n",
      "tensor(1.9692e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9973, 0.5516, 0.7305, 0.5268]], dtype=torch.float64)\n",
      "tensor([[0.7412, 0.5122]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7418, 0.5074]], dtype=torch.float64)\n",
      "tensor(1.1619e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9972, 0.5529, 0.7358, 0.5274]], dtype=torch.float64)\n",
      "tensor([[0.7461, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7473, 0.5075]], dtype=torch.float64)\n",
      "tensor(1.6675e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9975, 0.5495, 0.7411, 0.5270]], dtype=torch.float64)\n",
      "tensor([[0.7509, 0.5122]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7527, 0.5076]], dtype=torch.float64)\n",
      "tensor(1.2560e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9974, 0.5512, 0.7464, 0.5227]], dtype=torch.float64)\n",
      "tensor([[0.7554, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7582, 0.5056]], dtype=torch.float64)\n",
      "tensor(6.8318e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9972, 0.5528, 0.7518, 0.5218]], dtype=torch.float64)\n",
      "tensor([[0.7603, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7636, 0.5056]], dtype=torch.float64)\n",
      "tensor(7.0164e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9975, 0.5496, 0.7571, 0.5228]], dtype=torch.float64)\n",
      "tensor([[0.7651, 0.5079]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7691, 0.5060]], dtype=torch.float64)\n",
      "tensor(9.5052e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9974, 0.5512, 0.7625, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.7702, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7745, 0.5061]], dtype=torch.float64)\n",
      "tensor(1.2182e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9977, 0.5480, 0.7678, 0.5236]], dtype=torch.float64)\n",
      "tensor([[0.7751, 0.5085]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7800, 0.5062]], dtype=torch.float64)\n",
      "tensor(1.4982e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9976, 0.5494, 0.7732, 0.5256]], dtype=torch.float64)\n",
      "tensor([[0.7801, 0.5109]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7855, 0.5059]], dtype=torch.float64)\n",
      "tensor(2.7178e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9978, 0.5463, 0.7783, 0.5264]], dtype=torch.float64)\n",
      "tensor([[0.7849, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7909, 0.5059]], dtype=torch.float64)\n",
      "tensor(3.3246e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9977, 0.5478, 0.7836, 0.5239]], dtype=torch.float64)\n",
      "tensor([[0.7897, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7964, 0.5060]], dtype=torch.float64)\n",
      "tensor(2.6128e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9980, 0.5449, 0.7890, 0.5240]], dtype=torch.float64)\n",
      "tensor([[0.7947, 0.5085]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8018, 0.5061]], dtype=torch.float64)\n",
      "tensor(2.8276e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 115/665 [00:01<00:05, 99.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9979, 0.5462, 0.7944, 0.5261]], dtype=torch.float64)\n",
      "tensor([[0.7999, 0.5110]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8073, 0.5061]], dtype=torch.float64)\n",
      "tensor(3.9147e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9977, 0.5479, 0.8000, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.8049, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8127, 0.5054]], dtype=torch.float64)\n",
      "tensor(3.2738e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9980, 0.5448, 0.8057, 0.5258]], dtype=torch.float64)\n",
      "tensor([[0.8103, 0.5102]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8182, 0.5054]], dtype=torch.float64)\n",
      "tensor(4.2838e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9978, 0.5465, 0.8112, 0.5233]], dtype=torch.float64)\n",
      "tensor([[0.8153, 0.5075]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8236, 0.5055]], dtype=torch.float64)\n",
      "tensor(3.6905e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.5436, 0.8164, 0.5231]], dtype=torch.float64)\n",
      "tensor([[0.8201, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8291, 0.5060]], dtype=torch.float64)\n",
      "tensor(4.1035e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9980, 0.5452, 0.8218, 0.5234]], dtype=torch.float64)\n",
      "tensor([[0.8251, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8345, 0.5061]], dtype=torch.float64)\n",
      "tensor(4.5061e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9982, 0.5425, 0.8274, 0.5231]], dtype=torch.float64)\n",
      "tensor([[0.8304, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8400, 0.5057]], dtype=torch.float64)\n",
      "tensor(4.6601e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.5440, 0.8329, 0.5228]], dtype=torch.float64)\n",
      "tensor([[0.8357, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8455, 0.5057]], dtype=torch.float64)\n",
      "tensor(4.7987e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.5412, 0.8384, 0.5248]], dtype=torch.float64)\n",
      "tensor([[0.8411, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8509, 0.5051]], dtype=torch.float64)\n",
      "tensor(5.0527e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9982, 0.5428, 0.8439, 0.5235]], dtype=torch.float64)\n",
      "tensor([[0.8466, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8564, 0.5051]], dtype=torch.float64)\n",
      "tensor(4.7428e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9984, 0.5401, 0.8495, 0.5222]], dtype=torch.float64)\n",
      "tensor([[0.8524, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8618, 0.5052]], dtype=torch.float64)\n",
      "tensor(4.5655e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.5414, 0.8548, 0.5261]], dtype=torch.float64)\n",
      "tensor([[0.8583, 0.5079]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8673, 0.5056]], dtype=torch.float64)\n",
      "tensor(4.2535e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9985, 0.5388, 0.8601, 0.5244]], dtype=torch.float64)\n",
      "tensor([[0.8642, 0.5055]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8727, 0.5057]], dtype=torch.float64)\n",
      "tensor(3.6430e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.5362, 0.8654, 0.5252]], dtype=torch.float64)\n",
      "tensor([[0.8704, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8782, 0.5051]], dtype=torch.float64)\n",
      "tensor(3.0613e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.5373, 0.8708, 0.5275]], dtype=torch.float64)\n",
      "tensor([[0.8771, 0.5085]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8836, 0.5051]], dtype=torch.float64)\n",
      "tensor(2.7087e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.5349, 0.8760, 0.5253]], dtype=torch.float64)\n",
      "tensor([[0.8844, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8891, 0.5052]], dtype=torch.float64)\n",
      "tensor(1.0982e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.5362, 0.8813, 0.5249]], dtype=torch.float64)\n",
      "tensor([[0.8923, 0.5052]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8945, 0.5040]], dtype=torch.float64)\n",
      "tensor(3.3822e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.5340, 0.8865, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.9008, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9000, 0.5040]], dtype=torch.float64)\n",
      "tensor(8.5745e-07, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.5352, 0.8918, 0.5243]], dtype=torch.float64)\n",
      "tensor([[0.9093, 0.5045]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9055, 0.5042]], dtype=torch.float64)\n",
      "tensor(7.5212e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9989, 0.5330, 0.8970, 0.5227]], dtype=torch.float64)\n",
      "tensor([[0.9187, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9109, 0.5043]], dtype=torch.float64)\n",
      "tensor(3.2031e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.5343, 0.9022, 0.5227]], dtype=torch.float64)\n",
      "tensor([[0.9276, 0.5023]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9164, 0.5043]], dtype=torch.float64)\n",
      "tensor(6.4506e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.5319, 0.9075, 0.5252]], dtype=torch.float64)\n",
      "tensor([[0.9357, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9218, 0.5038]], dtype=torch.float64)\n",
      "tensor(9.7726e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9989, 0.5332, 0.9128, 0.5234]], dtype=torch.float64)\n",
      "tensor([[0.9441, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9273, 0.5038]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.5310, 0.9182, 0.5225]], dtype=torch.float64)\n",
      "tensor([[0.9517, 0.5014]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9327, 0.5032]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.5288, 0.9237, 0.5232]], dtype=torch.float64)\n",
      "tensor([[0.9585, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9382, 0.5032]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.5301, 0.9292, 0.5222]], dtype=torch.float64)\n",
      "tensor([[0.9645, 0.5007]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9436, 0.5033]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 126/665 [00:01<00:05, 100.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9992, 0.5280, 0.9347, 0.5233]], dtype=torch.float64)\n",
      "tensor([[0.9697, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9491, 0.4932]], dtype=torch.float64)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.5294, 0.9403, 0.5210]], dtype=torch.float64)\n",
      "tensor([[0.9744, 0.4991]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9545, 0.4932]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.5273, 0.9457, 0.5179]], dtype=torch.float64)\n",
      "tensor([[0.9784, 0.4952]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9600, 0.4921]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.5287, 0.9511, 0.5182]], dtype=torch.float64)\n",
      "tensor([[0.9816, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9655, 0.4920]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.5270, 0.9566, 0.5164]], dtype=torch.float64)\n",
      "tensor([[0.9845, 0.4934]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9709, 0.4919]], dtype=torch.float64)\n",
      "tensor(9.3614e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.5253, 0.9620, 0.5171]], dtype=torch.float64)\n",
      "tensor([[0.9869, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9764, 0.4918]], dtype=torch.float64)\n",
      "tensor(5.8659e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.5268, 0.9674, 0.5157]], dtype=torch.float64)\n",
      "tensor([[0.9889, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9818, 0.4917]], dtype=torch.float64)\n",
      "tensor(2.5423e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.5253, 0.9727, 0.5160]], dtype=torch.float64)\n",
      "tensor([[0.9905, 0.4936]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9873, 0.4916]], dtype=torch.float64)\n",
      "tensor(7.1159e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.5268, 0.9780, 0.5158]], dtype=torch.float64)\n",
      "tensor([[0.9919, 0.4933]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9927, 0.4921]], dtype=torch.float64)\n",
      "tensor(1.1080e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.5252, 0.9834, 0.5181]], dtype=torch.float64)\n",
      "tensor([[0.9931, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9982, 0.4920]], dtype=torch.float64)\n",
      "tensor(2.3200e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.5237, 0.9889, 0.5182]], dtype=torch.float64)\n",
      "tensor([[0.9942, 0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4911]], dtype=torch.float64)\n",
      "tensor(3.3805e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.5249, 0.9925, 0.5196]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4911]], dtype=torch.float64)\n",
      "tensor(4.3172e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.5233, 0.9941, 0.5197]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4991]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4911]], dtype=torch.float64)\n",
      "tensor(4.3918e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.5220, 0.9950, 0.5175]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4964]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4907]], dtype=torch.float64)\n",
      "tensor(2.8298e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.5231, 0.9955, 0.5207]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4907]], dtype=torch.float64)\n",
      "tensor(5.8676e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.5215, 0.9958, 0.5218]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4891]], dtype=torch.float64)\n",
      "tensor(9.2132e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5202, 0.9961, 0.5197]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4993]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4891]], dtype=torch.float64)\n",
      "tensor(6.3018e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.5213, 0.9963, 0.5176]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4966]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(4.2506e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5199, 0.9959, 0.5191]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4986]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(6.0806e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5191, 0.9955, 0.5136]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(1.5847e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 148/665 [00:01<00:05, 99.83it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9996, 0.5203, 0.9951, 0.5135]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4882]], dtype=torch.float64)\n",
      "tensor(1.6971e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5193, 0.9952, 0.5142]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4924]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4882]], dtype=torch.float64)\n",
      "tensor(2.0473e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5206, 0.9952, 0.5133]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4882]], dtype=torch.float64)\n",
      "tensor(1.5893e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5197, 0.9953, 0.5139]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4921]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4891]], dtype=torch.float64)\n",
      "tensor(1.5716e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5188, 0.9953, 0.5137]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4891]], dtype=torch.float64)\n",
      "tensor(1.4827e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5202, 0.9954, 0.5133]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(1.4699e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5202, 0.9954, 0.5133]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(1.4699e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5194, 0.9952, 0.5130]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(1.3813e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.5186, 0.9952, 0.5131]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(1.4285e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5199, 0.9951, 0.5138]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4888]], dtype=torch.float64)\n",
      "tensor(1.6182e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5191, 0.9952, 0.5160]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4888]], dtype=torch.float64)\n",
      "tensor(2.8715e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.5183, 0.9951, 0.5154]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(2.5698e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5193, 0.9951, 0.5188]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4981]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(5.6985e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5188, 0.9950, 0.5138]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4920]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4886]], dtype=torch.float64)\n",
      "tensor(1.7152e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5200, 0.9950, 0.5146]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4929]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4892]], dtype=torch.float64)\n",
      "tensor(1.8402e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5193, 0.9949, 0.5159]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4884]], dtype=torch.float64)\n",
      "tensor(3.0759e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5188, 0.9948, 0.5133]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4432]], dtype=torch.float64)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5203, 0.9952, 0.5111]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4432]], dtype=torch.float64)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.5154, 0.9952, 0.4903]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4630]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4432]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5111, 0.9950, 0.4769]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4470]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4443]], dtype=torch.float64)\n",
      "tensor(1.3282e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5072, 0.9953, 0.4730]], dtype=torch.float64)\n",
      "tensor([[0.9957, 0.4424]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4443]], dtype=torch.float64)\n",
      "tensor(1.1194e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5203, 0.9955, 0.4720]], dtype=torch.float64)\n",
      "tensor([[0.9957, 0.4406]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4459]], dtype=torch.float64)\n",
      "tensor(2.3310e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 171/665 [00:01<00:04, 103.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9997, 0.5160, 0.9953, 0.4749]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4442]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4459]], dtype=torch.float64)\n",
      "tensor(1.0978e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.5225, 0.9946, 0.4720]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4405]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4459]], dtype=torch.float64)\n",
      "tensor(2.4452e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.5259, 0.9944, 0.4720]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4403]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4531]], dtype=torch.float64)\n",
      "tensor(9.1132e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.5221, 0.9943, 0.4720]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4404]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4531]], dtype=torch.float64)\n",
      "tensor(8.9654e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.5359, 0.9943, 0.4781]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4471]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4363]], dtype=torch.float64)\n",
      "tensor(6.9097e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.5317, 0.9941, 0.4764]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4453]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4363]], dtype=torch.float64)\n",
      "tensor(5.1511e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.5220, 0.9942, 0.4662]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4336]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4363]], dtype=torch.float64)\n",
      "tensor(1.3668e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.5182, 0.9942, 0.4609]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4274]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4373]], dtype=torch.float64)\n",
      "tensor(5.8282e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.5147, 0.9942, 0.4591]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4255]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4373]], dtype=torch.float64)\n",
      "tensor(7.9025e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.5182, 0.9942, 0.4615]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4281]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4381]], dtype=torch.float64)\n",
      "tensor(5.9219e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.5146, 0.9941, 0.4642]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4315]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4381]], dtype=torch.float64)\n",
      "tensor(3.1550e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5200, 0.9940, 0.4650]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4322]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4381]], dtype=torch.float64)\n",
      "tensor(2.7244e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.5163, 0.9940, 0.4661]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4336]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4451]], dtype=torch.float64)\n",
      "tensor(7.5634e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5198, 0.9942, 0.4647]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4319]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4451]], dtype=torch.float64)\n",
      "tensor(9.6531e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.5287, 0.9944, 0.4720]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4402]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4508]], dtype=torch.float64)\n",
      "tensor(6.6009e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.5324, 0.9945, 0.4737]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4420]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4508]], dtype=torch.float64)\n",
      "tensor(4.8218e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.5413, 0.9945, 0.4773]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4459]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4508]], dtype=torch.float64)\n",
      "tensor(2.1985e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.5370, 0.9944, 0.4769]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4456]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4561]], dtype=torch.float64)\n",
      "tensor(6.5528e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.5408, 0.9941, 0.4786]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4475]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4561]], dtype=torch.float64)\n",
      "tensor(4.8042e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9975, 0.5499, 0.9935, 0.4828]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4521]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.9195]], dtype=torch.float64)\n",
      "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9971, 0.5541, 0.9937, 0.4847]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4542]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.9152]], dtype=torch.float64)\n",
      "tensor(0.1063, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 194/665 [00:02<00:04, 107.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8470, 0.8600, 0.9937, 0.5399]], dtype=torch.float64)\n",
      "tensor([[0.9964, 0.5678]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.9152]], dtype=torch.float64)\n",
      "tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.8413, 0.8654, 0.9938, 0.5779]], dtype=torch.float64)\n",
      "tensor([[0.9959, 0.6198]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.9209]], dtype=torch.float64)\n",
      "tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.8322, 0.8737, 0.9938, 0.6147]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.6671]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.9209]], dtype=torch.float64)\n",
      "tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.8466, 0.8603, 0.9937, 0.6490]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.7022]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3440]], dtype=torch.float64)\n",
      "tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.8400, 0.8666, 0.9937, 0.6801]], dtype=torch.float64)\n",
      "tensor([[0.9942, 0.7347]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3440]], dtype=torch.float64)\n",
      "tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9857, 0.3814, 0.9937, 0.6956]], dtype=torch.float64)\n",
      "tensor([[0.9917, 0.6832]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3440]], dtype=torch.float64)\n",
      "tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9824, 0.3684, 0.9937, 0.6898]], dtype=torch.float64)\n",
      "tensor([[0.9919, 0.6744]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3213]], dtype=torch.float64)\n",
      "tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9773, 0.3510, 0.9936, 0.6642]], dtype=torch.float64)\n",
      "tensor([[0.9927, 0.6468]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3213]], dtype=torch.float64)\n",
      "tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9838, 0.3739, 0.9936, 0.6193]], dtype=torch.float64)\n",
      "tensor([[0.9939, 0.6137]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3050]], dtype=torch.float64)\n",
      "tensor(0.0476, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9802, 0.3607, 0.9936, 0.5746]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5648]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3050]], dtype=torch.float64)\n",
      "tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9846, 0.3768, 0.9936, 0.5192]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3050]], dtype=torch.float64)\n",
      "tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9838, 0.3737, 0.9935, 0.4874]], dtype=torch.float64)\n",
      "tensor([[0.9959, 0.4635]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.2972]], dtype=torch.float64)\n",
      "tensor(0.0138, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9816, 0.3657, 0.9937, 0.4514]], dtype=torch.float64)\n",
      "tensor([[0.9961, 0.4168]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.2972]], dtype=torch.float64)\n",
      "tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9876, 0.3894, 0.9938, 0.4105]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3703]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.2969]], dtype=torch.float64)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9862, 0.3832, 0.9939, 0.3919]], dtype=torch.float64)\n",
      "tensor([[0.9963, 0.3485]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.2969]], dtype=torch.float64)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9850, 0.3783, 0.9940, 0.3748]], dtype=torch.float64)\n",
      "tensor([[0.9964, 0.3290]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.2969]], dtype=torch.float64)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9855, 0.3805, 0.9939, 0.3613]], dtype=torch.float64)\n",
      "tensor([[0.9963, 0.3146]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3147]], dtype=torch.float64)\n",
      "tensor(6.6799e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9849, 0.3781, 0.9938, 0.3426]], dtype=torch.float64)\n",
      "tensor([[0.9963, 0.2948]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3147]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9907, 0.4039, 0.9937, 0.3527]], dtype=torch.float64)\n",
      "tensor([[0.9963, 0.3076]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3280]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9900, 0.4003, 0.9937, 0.3554]], dtype=torch.float64)\n",
      "tensor([[0.9963, 0.3101]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3280]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9934, 0.4191, 0.9938, 0.3625]], dtype=torch.float64)\n",
      "tensor([[0.9963, 0.3189]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3280]], dtype=torch.float64)\n",
      "tensor(4.8095e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9940, 0.4225, 0.9937, 0.3639]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3206]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3340]], dtype=torch.float64)\n",
      "tensor(9.6118e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9934, 0.4193, 0.9936, 0.3648]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3214]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3340]], dtype=torch.float64)\n",
      "tensor(8.5804e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 218/665 [00:02<00:04, 109.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9891, 0.3964, 0.9934, 0.3700]], dtype=torch.float64)\n",
      "tensor([[0.9963, 0.3255]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3453]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9896, 0.3983, 0.9933, 0.3715]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3273]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3453]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9892, 0.3965, 0.9932, 0.3809]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3375]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3453]], dtype=torch.float64)\n",
      "tensor(3.7683e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9894, 0.3977, 0.9931, 0.3856]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3427]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3594]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9893, 0.3971, 0.9931, 0.3825]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3392]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3594]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9932, 0.4181, 0.9931, 0.3921]], dtype=torch.float64)\n",
      "tensor([[0.9961, 0.3510]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3451]], dtype=torch.float64)\n",
      "tensor(2.5211e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9935, 0.4199, 0.9931, 0.3950]], dtype=torch.float64)\n",
      "tensor([[0.9961, 0.3543]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3451]], dtype=torch.float64)\n",
      "tensor(4.9895e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9894, 0.3978, 0.9931, 0.4003]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3592]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3451]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9887, 0.3945, 0.9931, 0.4100]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3701]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3624]], dtype=torch.float64)\n",
      "tensor(3.6656e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9881, 0.3917, 0.9927, 0.4142]], dtype=torch.float64)\n",
      "tensor([[0.9961, 0.3748]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3624]], dtype=torch.float64)\n",
      "tensor(8.4126e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9932, 0.4178, 0.9923, 0.4146]], dtype=torch.float64)\n",
      "tensor([[0.9960, 0.3763]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3743]], dtype=torch.float64)\n",
      "tensor(9.9833e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9928, 0.4156, 0.9923, 0.4019]], dtype=torch.float64)\n",
      "tensor([[0.9960, 0.3619]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3743]], dtype=torch.float64)\n",
      "tensor(8.5220e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9976, 0.4507, 0.9927, 0.3993]], dtype=torch.float64)\n",
      "tensor([[0.9960, 0.3594]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3743]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9973, 0.4478, 0.9929, 0.4006]], dtype=torch.float64)\n",
      "tensor([[0.9960, 0.3609]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3680]], dtype=torch.float64)\n",
      "tensor(3.3133e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9975, 0.4505, 0.9933, 0.4057]], dtype=torch.float64)\n",
      "tensor([[0.9960, 0.3666]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3687]], dtype=torch.float64)\n",
      "tensor(1.0228e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9902, 0.4017, 0.9931, 0.4019]], dtype=torch.float64)\n",
      "tensor([[0.9962, 0.3613]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3895]], dtype=torch.float64)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9902, 0.4016, 0.9928, 0.4036]], dtype=torch.float64)\n",
      "tensor([[0.9961, 0.3632]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.3905]], dtype=torch.float64)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9968, 0.4432, 0.9927, 0.4304]], dtype=torch.float64)\n",
      "tensor([[0.9959, 0.3944]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4054]], dtype=torch.float64)\n",
      "tensor(6.9794e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9965, 0.4409, 0.9928, 0.4406]], dtype=torch.float64)\n",
      "tensor([[0.9959, 0.4062]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4054]], dtype=torch.float64)\n",
      "tensor(8.9256e-06, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4635, 0.9930, 0.4528]], dtype=torch.float64)\n",
      "tensor([[0.9957, 0.4201]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4128]], dtype=torch.float64)\n",
      "tensor(3.5700e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.4647, 0.9932, 0.4555]], dtype=torch.float64)\n",
      "tensor([[0.9957, 0.4233]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4128]], dtype=torch.float64)\n",
      "tensor(6.4230e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4882, 0.9933, 0.4568]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4238]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4137]], dtype=torch.float64)\n",
      "tensor(6.0665e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4915, 0.9933, 0.4458]], dtype=torch.float64)\n",
      "tensor([[0.9957, 0.4107]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4082]], dtype=torch.float64)\n",
      "tensor(1.2349e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4885, 0.9930, 0.4430]], dtype=torch.float64)\n",
      "tensor([[0.9957, 0.4076]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4082]], dtype=torch.float64)\n",
      "tensor(9.6189e-06, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 242/665 [00:02<00:03, 111.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9973, 0.4479, 0.9927, 0.4383]], dtype=torch.float64)\n",
      "tensor([[0.9958, 0.4033]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4206]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9973, 0.4486, 0.9926, 0.4476]], dtype=torch.float64)\n",
      "tensor([[0.9958, 0.4144]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4206]], dtype=torch.float64)\n",
      "tensor(2.8451e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4729, 0.9920, 0.4648]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4341]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4206]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4692, 0.9920, 0.4810]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4540]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4282]], dtype=torch.float64)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4680, 0.9921, 0.4994]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4282]], dtype=torch.float64)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4920, 0.9921, 0.5180]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4175]], dtype=torch.float64)\n",
      "tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4917, 0.9921, 0.5203]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4155]], dtype=torch.float64)\n",
      "tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9968, 0.4438, 0.9920, 0.5061]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4872]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4155]], dtype=torch.float64)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9967, 0.4425, 0.9923, 0.4817]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4563]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4183]], dtype=torch.float64)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9967, 0.4429, 0.9923, 0.4573]], dtype=torch.float64)\n",
      "tensor([[0.9957, 0.4262]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4204]], dtype=torch.float64)\n",
      "tensor(2.6087e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4744, 0.9923, 0.4506]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4169]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4295]], dtype=torch.float64)\n",
      "tensor(8.8492e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4728, 0.9923, 0.4589]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4270]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4295]], dtype=torch.float64)\n",
      "tensor(1.3143e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5057, 0.9924, 0.4718]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4410]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4295]], dtype=torch.float64)\n",
      "tensor(7.6773e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5022, 0.9927, 0.4772]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4477]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4287]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4990, 0.9929, 0.4813]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4528]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4287]], dtype=torch.float64)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4907, 0.9929, 0.4767]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4477]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4289]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4893, 0.9930, 0.4676]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4367]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4289]], dtype=torch.float64)\n",
      "tensor(4.0220e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4852, 0.9931, 0.4584]], dtype=torch.float64)\n",
      "tensor([[0.9956, 0.4258]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4289]], dtype=torch.float64)\n",
      "tensor(1.4578e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4846, 0.9926, 0.4548]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4215]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4284]], dtype=torch.float64)\n",
      "tensor(3.3318e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4844, 0.9924, 0.4556]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4226]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4284]], dtype=torch.float64)\n",
      "tensor(2.6823e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 254/665 [00:02<00:04, 94.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.4767, 0.9923, 0.4542]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4212]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4364]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.4774, 0.9920, 0.4552]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4223]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4367]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5045, 0.9918, 0.4625]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4298]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4367]], dtype=torch.float64)\n",
      "tensor(3.4815e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5038, 0.9918, 0.4632]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4307]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4496]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5064, 0.9921, 0.4672]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4353]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4462]], dtype=torch.float64)\n",
      "tensor(7.0190e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.5251, 0.9920, 0.4818]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4520]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4460]], dtype=torch.float64)\n",
      "tensor(3.0189e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.5279, 0.9921, 0.4853]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4561]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4460]], dtype=torch.float64)\n",
      "tensor(6.3478e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4974, 0.9921, 0.4755]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4458]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4489]], dtype=torch.float64)\n",
      "tensor(1.5999e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4980, 0.9919, 0.4689]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4378]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4517]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4997, 0.9919, 0.4761]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4463]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4522]], dtype=torch.float64)\n",
      "tensor(2.8716e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5194, 0.9919, 0.4761]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4454]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4522]], dtype=torch.float64)\n",
      "tensor(3.5145e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.5175, 0.9921, 0.4848]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4560]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4489]], dtype=torch.float64)\n",
      "tensor(3.7670e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.5158, 0.9923, 0.4910]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4637]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4361]], dtype=torch.float64)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4740, 0.9925, 0.4915]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4668]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4361]], dtype=torch.float64)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4734, 0.9927, 0.4817]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4546]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4408]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4746, 0.9926, 0.4760]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4476]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4404]], dtype=torch.float64)\n",
      "tensor(3.5923e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4747, 0.9929, 0.4715]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4422]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4477]], dtype=torch.float64)\n",
      "tensor(2.5282e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4748, 0.9929, 0.4691]], dtype=torch.float64)\n",
      "tensor([[0.9955, 0.4392]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4478]], dtype=torch.float64)\n",
      "tensor(4.7174e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 275/665 [00:03<00:04, 86.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4985, 0.9924, 0.4731]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4429]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4478]], dtype=torch.float64)\n",
      "tensor(2.3168e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4972, 0.9922, 0.4751]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4453]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4487]], dtype=torch.float64)\n",
      "tensor(1.6907e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4963, 0.9921, 0.4734]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4433]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4488]], dtype=torch.float64)\n",
      "tensor(2.6189e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4940, 0.9921, 0.4723]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4421]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4483]], dtype=torch.float64)\n",
      "tensor(3.0644e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4935, 0.9920, 0.4716]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4412]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4484]], dtype=torch.float64)\n",
      "tensor(3.6976e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4935, 0.9919, 0.4722]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4419]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4487]], dtype=torch.float64)\n",
      "tensor(3.4352e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4956, 0.9918, 0.4702]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4395]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4646]], dtype=torch.float64)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4955, 0.9921, 0.4736]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4436]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4654]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4894, 0.9922, 0.4861]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4591]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4676]], dtype=torch.float64)\n",
      "tensor(4.7679e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4901, 0.9922, 0.4918]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4662]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4676]], dtype=torch.float64)\n",
      "tensor(1.2802e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5069, 0.9922, 0.4896]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4624]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4676]], dtype=torch.float64)\n",
      "tensor(2.5765e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5083, 0.9922, 0.4948]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4687]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4716]], dtype=torch.float64)\n",
      "tensor(1.6504e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5061, 0.9923, 0.4998]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4751]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4727]], dtype=torch.float64)\n",
      "tensor(1.5316e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4888, 0.9925, 0.5073]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4856]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4771]], dtype=torch.float64)\n",
      "tensor(4.8265e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 284/665 [00:03<00:04, 81.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9999, 0.4889, 0.9927, 0.5080]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4865]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4773]], dtype=torch.float64)\n",
      "tensor(5.4925e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5004, 0.9923, 0.5061]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4833]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4754]], dtype=torch.float64)\n",
      "tensor(4.3679e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4990, 0.9923, 0.5023]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4786]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4739]], dtype=torch.float64)\n",
      "tensor(2.3579e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5000, 0.9923, 0.5002]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4759]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4739]], dtype=torch.float64)\n",
      "tensor(1.4294e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4967, 0.9922, 0.5007]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4769]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4758]], dtype=torch.float64)\n",
      "tensor(1.2824e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4960, 0.9921, 0.5000]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4760]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4752]], dtype=torch.float64)\n",
      "tensor(1.2648e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4967, 0.9918, 0.5038]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.4807]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4754]], dtype=torch.float64)\n",
      "tensor(2.7024e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4942, 0.9918, 0.5017]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4782]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4739]], dtype=torch.float64)\n",
      "tensor(2.1769e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4950, 0.9918, 0.5009]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4739]], dtype=torch.float64)\n",
      "tensor(1.7729e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4923, 0.9919, 0.4980]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4747]], dtype=torch.float64)\n",
      "tensor(1.2823e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4921, 0.9920, 0.4971]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4726]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4674]], dtype=torch.float64)\n",
      "tensor(2.5498e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4741, 0.9918, 0.4988]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4759]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4676]], dtype=torch.float64)\n",
      "tensor(4.6351e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.4795, 0.9919, 0.4943]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4699]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5092]], dtype=torch.float64)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4873, 0.9919, 0.4932]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4680]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5097]], dtype=torch.float64)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4872, 0.9920, 0.5029]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4801]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5156]], dtype=torch.float64)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 302/665 [00:03<00:04, 75.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.5025, 0.9921, 0.5115]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.4900]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5177]], dtype=torch.float64)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4689, 0.9921, 0.5298]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5155]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5169]], dtype=torch.float64)\n",
      "tensor(1.4325e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9989, 0.4667, 0.9921, 0.5350]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5224]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5201]], dtype=torch.float64)\n",
      "tensor(1.6333e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4728, 0.9922, 0.5344]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5211]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5202]], dtype=torch.float64)\n",
      "tensor(1.4187e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4864, 0.9920, 0.5382]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5249]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5296]], dtype=torch.float64)\n",
      "tensor(2.5729e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4848, 0.9921, 0.5412]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5280]], dtype=torch.float64)\n",
      "tensor(1.5100e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4735, 0.9922, 0.5462]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5360]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5308]], dtype=torch.float64)\n",
      "tensor(2.8577e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.4713, 0.9923, 0.5430]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5322]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5212]], dtype=torch.float64)\n",
      "tensor(7.4677e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4686, 0.9921, 0.5482]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5389]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5212]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9989, 0.4665, 0.9920, 0.5401]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5251]], dtype=torch.float64)\n",
      "tensor(2.1276e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9925, 0.4140, 0.9920, 0.5403]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5322]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5223]], dtype=torch.float64)\n",
      "tensor(6.1668e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9916, 0.4089, 0.9920, 0.5431]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5357]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5236]], dtype=torch.float64)\n",
      "tensor(8.6072e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9892, 0.3965, 0.9921, 0.5435]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5359]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5411]], dtype=torch.float64)\n",
      "tensor(2.5679e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9879, 0.3905, 0.9916, 0.5435]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5448]], dtype=torch.float64)\n",
      "tensor(5.4190e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9865, 0.3846, 0.9916, 0.5542]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5477]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5053]], dtype=torch.float64)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9874, 0.3883, 0.9919, 0.5630]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5582]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5183]], dtype=torch.float64)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 321/665 [00:03<00:04, 81.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9785, 0.3548, 0.9918, 0.5343]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.5176]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5064]], dtype=torch.float64)\n",
      "tensor(7.4441e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9893, 0.3970, 0.9919, 0.5350]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.5255]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5325]], dtype=torch.float64)\n",
      "tensor(3.6420e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9882, 0.3918, 0.9919, 0.5301]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.5193]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5333]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9900, 0.4006, 0.9919, 0.5308]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5137]], dtype=torch.float64)\n",
      "tensor(3.4540e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9900, 0.4006, 0.9919, 0.5308]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5137]], dtype=torch.float64)\n",
      "tensor(3.4540e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9912, 0.4067, 0.9918, 0.5363]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5366]], dtype=torch.float64)\n",
      "tensor(5.6089e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9944, 0.4254, 0.9920, 0.5356]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5258]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5232]], dtype=torch.float64)\n",
      "tensor(1.6046e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9929, 0.4158, 0.9919, 0.5466]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5399]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5107]], dtype=torch.float64)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9920, 0.4112, 0.9917, 0.5398]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5316]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5111]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9919, 0.4104, 0.9916, 0.5317]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5559]], dtype=torch.float64)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9927, 0.4149, 0.9923, 0.5321]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.5218]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7217]], dtype=torch.float64)\n",
      "tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9971, 0.4462, 0.9922, 0.5593]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5542]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7309]], dtype=torch.float64)\n",
      "tensor(0.0156, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9962, 0.4384, 0.9920, 0.6011]], dtype=torch.float64)\n",
      "tensor([[0.9938, 0.6033]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7858]], dtype=torch.float64)\n",
      "tensor(0.0167, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9958, 0.4350, 0.9919, 0.6502]], dtype=torch.float64)\n",
      "tensor([[0.9926, 0.6539]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7858]], dtype=torch.float64)\n",
      "tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9975, 0.4501, 0.9920, 0.6988]], dtype=torch.float64)\n",
      "tensor([[0.9911, 0.6990]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7808]], dtype=torch.float64)\n",
      "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.4648, 0.9922, 0.7435]], dtype=torch.float64)\n",
      "tensor([[0.9895, 0.7374]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7748]], dtype=torch.float64)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4643, 0.9923, 0.7685]], dtype=torch.float64)\n",
      "tensor([[0.9886, 0.7564]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7748]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9976, 0.4506, 0.9920, 0.7743]], dtype=torch.float64)\n",
      "tensor([[0.9883, 0.7581]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7506]], dtype=torch.float64)\n",
      "tensor(9.6291e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9976, 0.4506, 0.9920, 0.7738]], dtype=torch.float64)\n",
      "tensor([[0.9883, 0.7578]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7506]], dtype=torch.float64)\n",
      "tensor(9.3900e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9980, 0.4550, 0.9918, 0.7667]], dtype=torch.float64)\n",
      "tensor([[0.9885, 0.7535]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7623]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 339/665 [00:03<00:04, 77.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9966, 0.4415, 0.9918, 0.7652]], dtype=torch.float64)\n",
      "tensor([[0.9886, 0.7499]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7578]], dtype=torch.float64)\n",
      "tensor(9.5869e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9967, 0.4429, 0.9917, 0.7593]], dtype=torch.float64)\n",
      "tensor([[0.9888, 0.7459]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7578]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9977, 0.4524, 0.9917, 0.7534]], dtype=torch.float64)\n",
      "tensor([[0.9890, 0.7431]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6947]], dtype=torch.float64)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4686, 0.9919, 0.7529]], dtype=torch.float64)\n",
      "tensor([[0.9890, 0.7452]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6814]], dtype=torch.float64)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.4778, 0.9917, 0.7261]], dtype=torch.float64)\n",
      "tensor([[0.9899, 0.7247]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6814]], dtype=torch.float64)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.4694, 0.9919, 0.7015]], dtype=torch.float64)\n",
      "tensor([[0.9908, 0.7030]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6844]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4732, 0.9920, 0.6872]], dtype=torch.float64)\n",
      "tensor([[0.9913, 0.6903]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6519]], dtype=torch.float64)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.4651, 0.9920, 0.6740]], dtype=torch.float64)\n",
      "tensor([[0.9917, 0.6778]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6391]], dtype=torch.float64)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4765, 0.9921, 0.6511]], dtype=torch.float64)\n",
      "tensor([[0.9923, 0.6557]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6391]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4884, 0.9914, 0.6321]], dtype=torch.float64)\n",
      "tensor([[0.9926, 0.6358]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7312]], dtype=torch.float64)\n",
      "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4979, 0.9916, 0.6357]], dtype=torch.float64)\n",
      "tensor([[0.9925, 0.6393]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7312]], dtype=torch.float64)\n",
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5027, 0.9915, 0.6586]], dtype=torch.float64)\n",
      "tensor([[0.9918, 0.6631]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7488]], dtype=torch.float64)\n",
      "tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4939, 0.9917, 0.6889]], dtype=torch.float64)\n",
      "tensor([[0.9910, 0.6928]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7354]], dtype=torch.float64)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4966, 0.9918, 0.7228]], dtype=torch.float64)\n",
      "tensor([[0.9899, 0.7235]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7282]], dtype=torch.float64)\n",
      "tensor(6.2238e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4977, 0.9915, 0.7383]], dtype=torch.float64)\n",
      "tensor([[0.9893, 0.7367]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7155]], dtype=torch.float64)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4882, 0.9916, 0.7301]], dtype=torch.float64)\n",
      "tensor([[0.9897, 0.7290]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7149]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 357/665 [00:04<00:03, 77.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9999, 0.4918, 0.9915, 0.7172]], dtype=torch.float64)\n",
      "tensor([[0.9901, 0.7183]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7219]], dtype=torch.float64)\n",
      "tensor(5.5873e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4941, 0.9917, 0.7085]], dtype=torch.float64)\n",
      "tensor([[0.9904, 0.7108]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7084]], dtype=torch.float64)\n",
      "tensor(4.9361e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4851, 0.9918, 0.7079]], dtype=torch.float64)\n",
      "tensor([[0.9905, 0.7097]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7040]], dtype=torch.float64)\n",
      "tensor(6.1702e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4976, 0.9919, 0.7042]], dtype=torch.float64)\n",
      "tensor([[0.9905, 0.7071]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7007]], dtype=torch.float64)\n",
      "tensor(6.5548e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5053, 0.9922, 0.7084]], dtype=torch.float64)\n",
      "tensor([[0.9904, 0.7114]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7197]], dtype=torch.float64)\n",
      "tensor(8.0175e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9976, 0.5487, 0.9922, 0.7089]], dtype=torch.float64)\n",
      "tensor([[0.9902, 0.7146]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7197]], dtype=torch.float64)\n",
      "tensor(6.0991e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9976, 0.5487, 0.9922, 0.7089]], dtype=torch.float64)\n",
      "tensor([[0.9902, 0.7146]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7197]], dtype=torch.float64)\n",
      "tensor(6.0991e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9967, 0.5571, 0.9924, 0.7209]], dtype=torch.float64)\n",
      "tensor([[0.9898, 0.7267]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7177]], dtype=torch.float64)\n",
      "tensor(9.2769e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9961, 0.5620, 0.9924, 0.7223]], dtype=torch.float64)\n",
      "tensor([[0.9898, 0.7284]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7159]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9969, 0.5558, 0.9925, 0.7196]], dtype=torch.float64)\n",
      "tensor([[0.9899, 0.7254]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7151]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9942, 0.5756, 0.9925, 0.7204]], dtype=torch.float64)\n",
      "tensor([[0.9898, 0.7278]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.7112]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9863, 0.6162, 0.9925, 0.7179]], dtype=torch.float64)\n",
      "tensor([[0.9899, 0.7293]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6933]], dtype=torch.float64)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9861, 0.6171, 0.9923, 0.7139]], dtype=torch.float64)\n",
      "tensor([[0.9899, 0.7255]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6799]], dtype=torch.float64)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9866, 0.6150, 0.9922, 0.6995]], dtype=torch.float64)\n",
      "tensor([[0.9903, 0.7109]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6780]], dtype=torch.float64)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9899, 0.5998, 0.9921, 0.6888]], dtype=torch.float64)\n",
      "tensor([[0.9906, 0.6982]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6780]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 375/665 [00:04<00:03, 78.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9942, 0.5761, 0.9918, 0.6850]], dtype=torch.float64)\n",
      "tensor([[0.9907, 0.6923]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6749]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9898, 0.6003, 0.9918, 0.6863]], dtype=torch.float64)\n",
      "tensor([[0.9906, 0.6957]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6749]], dtype=torch.float64)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9904, 0.5976, 0.9917, 0.6849]], dtype=torch.float64)\n",
      "tensor([[0.9907, 0.6938]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6411]], dtype=torch.float64)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9878, 0.6099, 0.9918, 0.6867]], dtype=torch.float64)\n",
      "tensor([[0.9906, 0.6971]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6411]], dtype=torch.float64)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9862, 0.6166, 0.9916, 0.6710]], dtype=torch.float64)\n",
      "tensor([[0.9910, 0.6808]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6099]], dtype=torch.float64)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9862, 0.6167, 0.9916, 0.6557]], dtype=torch.float64)\n",
      "tensor([[0.9915, 0.6635]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6493]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9801, 0.6398, 0.9918, 0.6298]], dtype=torch.float64)\n",
      "tensor([[0.9923, 0.6356]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.6065]], dtype=torch.float64)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9761, 0.6526, 0.9917, 0.6433]], dtype=torch.float64)\n",
      "tensor([[0.9919, 0.6535]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5773]], dtype=torch.float64)\n",
      "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9736, 0.6603, 0.9921, 0.6219]], dtype=torch.float64)\n",
      "tensor([[0.9926, 0.6287]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5773]], dtype=torch.float64)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9718, 0.6654, 0.9925, 0.5950]], dtype=torch.float64)\n",
      "tensor([[0.9933, 0.5952]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5574]], dtype=torch.float64)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9750, 0.6561, 0.9926, 0.5930]], dtype=torch.float64)\n",
      "tensor([[0.9933, 0.5913]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5458]], dtype=torch.float64)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9765, 0.6515, 0.9922, 0.5730]], dtype=torch.float64)\n",
      "tensor([[0.9937, 0.5644]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5326]], dtype=torch.float64)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9747, 0.6569, 0.9919, 0.5627]], dtype=torch.float64)\n",
      "tensor([[0.9938, 0.5518]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5173]], dtype=torch.float64)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9791, 0.6432, 0.9919, 0.5537]], dtype=torch.float64)\n",
      "tensor([[0.9940, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5173]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9854, 0.6201, 0.9919, 0.5383]], dtype=torch.float64)\n",
      "tensor([[0.9942, 0.5178]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5173]], dtype=torch.float64)\n",
      "tensor(1.6906e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9883, 0.6074, 0.9919, 0.5372]], dtype=torch.float64)\n",
      "tensor([[0.9942, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5127]], dtype=torch.float64)\n",
      "tensor(2.3788e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 385/665 [00:04<00:03, 81.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9876, 0.6106, 0.9919, 0.5381]], dtype=torch.float64)\n",
      "tensor([[0.9942, 0.5176]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5127]], dtype=torch.float64)\n",
      "tensor(2.8653e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9904, 0.5977, 0.9919, 0.5328]], dtype=torch.float64)\n",
      "tensor([[0.9943, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5106]], dtype=torch.float64)\n",
      "tensor(1.6521e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9924, 0.5867, 0.9920, 0.5333]], dtype=torch.float64)\n",
      "tensor([[0.9943, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5106]], dtype=torch.float64)\n",
      "tensor(1.7791e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9941, 0.5765, 0.9920, 0.5257]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5034]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5106]], dtype=torch.float64)\n",
      "tensor(4.1963e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9935, 0.5802, 0.9920, 0.5227]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.4995]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5118]], dtype=torch.float64)\n",
      "tensor(9.1330e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9950, 0.5707, 0.9917, 0.5237]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5012]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5118]], dtype=torch.float64)\n",
      "tensor(7.1697e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9962, 0.5614, 0.9914, 0.5293]], dtype=torch.float64)\n",
      "tensor([[0.9943, 0.5086]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5065]], dtype=torch.float64)\n",
      "tensor(1.8326e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9958, 0.5646, 0.9915, 0.5239]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5065]], dtype=torch.float64)\n",
      "tensor(2.7074e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9970, 0.5550, 0.9913, 0.5234]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5065]], dtype=torch.float64)\n",
      "tensor(2.7528e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9978, 0.5472, 0.9912, 0.5252]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5043]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5043]], dtype=torch.float64)\n",
      "tensor(1.5804e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9973, 0.5517, 0.9914, 0.5241]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5043]], dtype=torch.float64)\n",
      "tensor(1.6839e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.5439, 0.9915, 0.5231]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5022]], dtype=torch.float64)\n",
      "tensor(1.5335e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.5368, 0.9915, 0.5228]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5022]], dtype=torch.float64)\n",
      "tensor(1.5225e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.5364, 0.9914, 0.5205]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.4991]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5022]], dtype=torch.float64)\n",
      "tensor(1.9857e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.5296, 0.9915, 0.5233]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4983]], dtype=torch.float64)\n",
      "tensor(2.6744e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.5189, 0.9916, 0.5232]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5036]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4983]], dtype=torch.float64)\n",
      "tensor(2.9047e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 405/665 [00:04<00:03, 84.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9996, 0.5200, 0.9918, 0.5209]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5006]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4931]], dtype=torch.float64)\n",
      "tensor(4.2704e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.5147, 0.9916, 0.5208]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4931]], dtype=torch.float64)\n",
      "tensor(4.4753e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5074, 0.9913, 0.5178]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.4976]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4931]], dtype=torch.float64)\n",
      "tensor(2.4330e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5076, 0.9913, 0.5218]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4862]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4967, 0.9913, 0.5216]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5030]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4862]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4964, 0.9916, 0.5241]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5062]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4827]], dtype=torch.float64)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4920, 0.9915, 0.5241]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5066]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4827]], dtype=torch.float64)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4815, 0.9914, 0.5199]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5021]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4827]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4815, 0.9915, 0.5103]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.4898]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4745]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4732, 0.9916, 0.5064]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4855]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4745]], dtype=torch.float64)\n",
      "tensor(7.3347e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.4705, 0.9917, 0.5053]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4844]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4728]], dtype=torch.float64)\n",
      "tensor(7.9035e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4689, 0.9917, 0.5173]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.4996]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4728]], dtype=torch.float64)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.4589, 0.9918, 0.5232]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5078]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4728]], dtype=torch.float64)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.4562, 0.9920, 0.5202]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4718]], dtype=torch.float64)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9979, 0.4542, 0.9923, 0.5206]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.5048]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4718]], dtype=torch.float64)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9977, 0.4519, 0.9925, 0.5183]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4685]], dtype=torch.float64)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9975, 0.4499, 0.9924, 0.5167]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4685]], dtype=torch.float64)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9967, 0.4423, 0.9920, 0.5055]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4864]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4685]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9966, 0.4420, 0.9918, 0.4967]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4655]], dtype=torch.float64)\n",
      "tensor(5.8027e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9957, 0.4343, 0.9916, 0.4927]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4705]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4655]], dtype=torch.float64)\n",
      "tensor(2.3300e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9956, 0.4341, 0.9915, 0.4916]], dtype=torch.float64)\n",
      "tensor([[0.9954, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4656]], dtype=torch.float64)\n",
      "tensor(1.6894e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 427/665 [00:04<00:02, 96.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9957, 0.4345, 0.9912, 0.4889]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4657]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4656]], dtype=torch.float64)\n",
      "tensor(1.0886e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9945, 0.4263, 0.9910, 0.4916]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4695]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4656]], dtype=torch.float64)\n",
      "tensor(1.8426e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9945, 0.4262, 0.9910, 0.4974]], dtype=torch.float64)\n",
      "tensor([[0.9953, 0.4769]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4954]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4946, 0.9915, 0.5061]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.4836]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4954]], dtype=torch.float64)\n",
      "tensor(8.2947e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4939, 0.9916, 0.5205]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4937]], dtype=torch.float64)\n",
      "tensor(4.7621e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4912, 0.9916, 0.5213]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4937]], dtype=torch.float64)\n",
      "tensor(5.8948e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4827, 0.9916, 0.5171]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4937]], dtype=torch.float64)\n",
      "tensor(2.4271e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4821, 0.9916, 0.5176]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.4990]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4891]], dtype=torch.float64)\n",
      "tensor(6.2228e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4736, 0.9915, 0.5182]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4891]], dtype=torch.float64)\n",
      "tensor(7.6721e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.4721, 0.9916, 0.5233]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4865]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.4700, 0.9917, 0.5223]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4865]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9985, 0.4618, 0.9920, 0.5229]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4865]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9984, 0.4601, 0.9923, 0.5188]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5022]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4858]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.4584, 0.9922, 0.5204]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5043]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4858]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9982, 0.4573, 0.9920, 0.5200]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4804]], dtype=torch.float64)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.4560, 0.9918, 0.5117]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4934]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4804]], dtype=torch.float64)\n",
      "tensor(9.6583e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9973, 0.4484, 0.9917, 0.5066]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4874]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4804]], dtype=torch.float64)\n",
      "tensor(3.6334e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9971, 0.4467, 0.9916, 0.5049]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4778]], dtype=torch.float64)\n",
      "tensor(4.0748e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9964, 0.4398, 0.9915, 0.5044]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4852]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4778]], dtype=torch.float64)\n",
      "tensor(3.9117e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9964, 0.4400, 0.9913, 0.5036]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4841]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4978]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9961, 0.4379, 0.9916, 0.5060]], dtype=torch.float64)\n",
      "tensor([[0.9952, 0.4873]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4978]], dtype=torch.float64)\n",
      "tensor(6.6885e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5018, 0.9916, 0.5211]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5021]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4978]], dtype=torch.float64)\n",
      "tensor(2.3457e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4991, 0.9916, 0.5222]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4959]], dtype=torch.float64)\n",
      "tensor(4.4459e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 449/665 [00:05<00:02, 101.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9999, 0.4929, 0.9916, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4959]], dtype=torch.float64)\n",
      "tensor(5.6502e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4901, 0.9917, 0.5245]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4933]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4881, 0.9916, 0.5199]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4933]], dtype=torch.float64)\n",
      "tensor(4.8079e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4817, 0.9916, 0.5148]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.4956]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4933]], dtype=torch.float64)\n",
      "tensor(1.5863e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.4796, 0.9914, 0.5174]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.4990]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4934]], dtype=torch.float64)\n",
      "tensor(2.8693e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.4776, 0.9915, 0.5225]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4934]], dtype=torch.float64)\n",
      "tensor(8.7947e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.4766, 0.9916, 0.5201]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5026]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4912]], dtype=torch.float64)\n",
      "tensor(7.8174e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4752, 0.9919, 0.5199]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4912]], dtype=torch.float64)\n",
      "tensor(7.6499e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9989, 0.4671, 0.9921, 0.5186]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4912]], dtype=torch.float64)\n",
      "tensor(6.3467e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.4658, 0.9918, 0.5211]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5046]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4866]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.4584, 0.9916, 0.5165]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4992]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4866]], dtype=torch.float64)\n",
      "tensor(9.1928e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.4569, 0.9915, 0.5136]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4957]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4849]], dtype=torch.float64)\n",
      "tensor(7.1323e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.4564, 0.9915, 0.5116]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4932]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4849]], dtype=torch.float64)\n",
      "tensor(4.7191e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9974, 0.4495, 0.9915, 0.5115]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4849]], dtype=torch.float64)\n",
      "tensor(4.9186e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9973, 0.4478, 0.9915, 0.5103]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4830]], dtype=torch.float64)\n",
      "tensor(5.4562e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9965, 0.4410, 0.9913, 0.5094]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4830]], dtype=torch.float64)\n",
      "tensor(4.7731e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9962, 0.4388, 0.9912, 0.5082]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4900]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5116]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9962, 0.4387, 0.9916, 0.5114]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5116]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5074, 0.9917, 0.5307]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5138]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5116]], dtype=torch.float64)\n",
      "tensor(1.7412e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5048, 0.9917, 0.5324]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5163]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5093]], dtype=torch.float64)\n",
      "tensor(3.9423e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4979, 0.9917, 0.5315]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5093]], dtype=torch.float64)\n",
      "tensor(3.4753e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4957, 0.9917, 0.5257]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5032]], dtype=torch.float64)\n",
      "tensor(2.7870e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4953, 0.9915, 0.5261]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5032]], dtype=torch.float64)\n",
      "tensor(3.0371e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4868, 0.9914, 0.5251]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5082]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5032]], dtype=torch.float64)\n",
      "tensor(2.6633e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4849, 0.9911, 0.5246]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4971]], dtype=torch.float64)\n",
      "tensor(6.9500e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 472/665 [00:05<00:02, 94.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.4779, 0.9912, 0.5220]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5049]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4971]], dtype=torch.float64)\n",
      "tensor(4.4226e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4762, 0.9911, 0.5194]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4965]], dtype=torch.float64)\n",
      "tensor(2.7320e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4748, 0.9910, 0.5212]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4965]], dtype=torch.float64)\n",
      "tensor(4.2763e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4737, 0.9911, 0.5209]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4965]], dtype=torch.float64)\n",
      "tensor(4.0339e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4727, 0.9912, 0.5209]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4910]], dtype=torch.float64)\n",
      "tensor(9.6008e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4641, 0.9912, 0.5219]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4910]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4634, 0.9911, 0.5186]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4894]], dtype=torch.float64)\n",
      "tensor(8.7568e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.4631, 0.9912, 0.5172]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.4998]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4894]], dtype=torch.float64)\n",
      "tensor(6.8196e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9979, 0.4544, 0.9913, 0.5162]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4992]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4894]], dtype=torch.float64)\n",
      "tensor(6.0856e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9979, 0.4544, 0.9914, 0.5129]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4950]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4878]], dtype=torch.float64)\n",
      "tensor(3.8526e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9972, 0.4475, 0.9912, 0.5133]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4878]], dtype=torch.float64)\n",
      "tensor(4.5954e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9970, 0.4454, 0.9911, 0.5131]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.4958]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5116]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9970, 0.4453, 0.9914, 0.5176]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5116]], dtype=torch.float64)\n",
      "tensor(6.2455e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5057, 0.9916, 0.5318]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5153]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5116]], dtype=torch.float64)\n",
      "tensor(2.2057e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5054, 0.9917, 0.5328]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5167]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5112]], dtype=torch.float64)\n",
      "tensor(2.9835e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5029, 0.9917, 0.5308]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5112]], dtype=torch.float64)\n",
      "tensor(1.9584e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 492/665 [00:05<00:01, 90.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.5009, 0.9915, 0.5278]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5087]], dtype=torch.float64)\n",
      "tensor(1.6583e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5007, 0.9913, 0.5268]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5087]], dtype=torch.float64)\n",
      "tensor(1.4985e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4919, 0.9912, 0.5240]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5087]], dtype=torch.float64)\n",
      "tensor(1.7129e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4902, 0.9911, 0.5270]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5104]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5051]], dtype=torch.float64)\n",
      "tensor(2.8663e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4823, 0.9909, 0.5281]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5123]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5051]], dtype=torch.float64)\n",
      "tensor(4.1170e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.4808, 0.9909, 0.5236]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5012]], dtype=torch.float64)\n",
      "tensor(2.9660e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.4797, 0.9910, 0.5244]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5078]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5012]], dtype=torch.float64)\n",
      "tensor(3.6638e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.4715, 0.9911, 0.5240]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5078]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5012]], dtype=torch.float64)\n",
      "tensor(3.6227e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.4705, 0.9913, 0.5212]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5005]], dtype=torch.float64)\n",
      "tensor(2.1042e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4690, 0.9914, 0.5229]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5005]], dtype=torch.float64)\n",
      "tensor(3.2728e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4685, 0.9915, 0.5201]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5032]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4975]], dtype=torch.float64)\n",
      "tensor(2.9484e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4682, 0.9916, 0.5223]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4975]], dtype=torch.float64)\n",
      "tensor(4.9937e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.4588, 0.9917, 0.5196]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5032]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4975]], dtype=torch.float64)\n",
      "tensor(2.9347e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.4585, 0.9918, 0.5216]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4916]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9974, 0.4488, 0.9914, 0.5201]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5046]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4916]], dtype=torch.float64)\n",
      "tensor(9.7040e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9974, 0.4487, 0.9913, 0.5197]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5127]], dtype=torch.float64)\n",
      "tensor(5.1113e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9974, 0.4488, 0.9915, 0.5225]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5127]], dtype=torch.float64)\n",
      "tensor(2.5895e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5098, 0.9917, 0.5333]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5170]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5127]], dtype=torch.float64)\n",
      "tensor(2.4262e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5071, 0.9917, 0.5314]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5107]], dtype=torch.float64)\n",
      "tensor(2.3364e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 514/665 [00:05<00:01, 98.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4983, 0.9916, 0.5279]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5110]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5107]], dtype=torch.float64)\n",
      "tensor(1.4648e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4980, 0.9914, 0.5270]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5080]], dtype=torch.float64)\n",
      "tensor(1.6450e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4958, 0.9912, 0.5248]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5080]], dtype=torch.float64)\n",
      "tensor(1.4990e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4872, 0.9911, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5055]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5080]], dtype=torch.float64)\n",
      "tensor(1.7458e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4861, 0.9911, 0.5288]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5076]], dtype=torch.float64)\n",
      "tensor(2.8927e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4842, 0.9911, 0.5282]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5076]], dtype=torch.float64)\n",
      "tensor(2.6080e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4829, 0.9912, 0.5252]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5038]], dtype=torch.float64)\n",
      "tensor(2.6373e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4819, 0.9911, 0.5252]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5038]], dtype=torch.float64)\n",
      "tensor(2.6538e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4732, 0.9912, 0.5235]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5038]], dtype=torch.float64)\n",
      "tensor(1.9746e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.4720, 0.9913, 0.5218]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4992]], dtype=torch.float64)\n",
      "tensor(3.0686e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4636, 0.9915, 0.5220]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4992]], dtype=torch.float64)\n",
      "tensor(3.5429e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.4630, 0.9916, 0.5204]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4969]], dtype=torch.float64)\n",
      "tensor(3.7379e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9985, 0.4613, 0.9916, 0.5202]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4969]], dtype=torch.float64)\n",
      "tensor(3.6136e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9978, 0.4534, 0.9918, 0.5181]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4969]], dtype=torch.float64)\n",
      "tensor(2.3769e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9978, 0.4531, 0.9917, 0.5183]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4946]], dtype=torch.float64)\n",
      "tensor(3.9691e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9968, 0.4433, 0.9915, 0.5235]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4946]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9968, 0.4431, 0.9915, 0.5229]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5086]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5134]], dtype=torch.float64)\n",
      "tensor(2.4587e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9964, 0.4402, 0.9917, 0.5247]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5110]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5134]], dtype=torch.float64)\n",
      "tensor(1.5475e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5098, 0.9917, 0.5302]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5134]], dtype=torch.float64)\n",
      "tensor(1.5062e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5078, 0.9917, 0.5242]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5109]], dtype=torch.float64)\n",
      "tensor(2.8407e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4984, 0.9914, 0.5273]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5102]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5109]], dtype=torch.float64)\n",
      "tensor(1.4981e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4980, 0.9912, 0.5282]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5085]], dtype=torch.float64)\n",
      "tensor(1.8874e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 534/665 [00:06<00:01, 89.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4964, 0.9912, 0.5243]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5065]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5085]], dtype=torch.float64)\n",
      "tensor(1.6552e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4883, 0.9911, 0.5282]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5085]], dtype=torch.float64)\n",
      "tensor(2.0958e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4863, 0.9911, 0.5288]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5046]], dtype=torch.float64)\n",
      "tensor(4.9213e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.4774, 0.9910, 0.5282]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5046]], dtype=torch.float64)\n",
      "tensor(4.7944e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4764, 0.9910, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5040]], dtype=torch.float64)\n",
      "tensor(1.6714e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4751, 0.9911, 0.5249]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5040]], dtype=torch.float64)\n",
      "tensor(2.5259e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4743, 0.9912, 0.5249]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5040]], dtype=torch.float64)\n",
      "tensor(2.5462e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4731, 0.9914, 0.5236]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5021]], dtype=torch.float64)\n",
      "tensor(2.7014e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4645, 0.9916, 0.5234]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5021]], dtype=torch.float64)\n",
      "tensor(2.8871e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.4628, 0.9918, 0.5205]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4977]], dtype=torch.float64)\n",
      "tensor(3.3390e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.4626, 0.9918, 0.5216]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5055]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4977]], dtype=torch.float64)\n",
      "tensor(4.3751e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9979, 0.4545, 0.9917, 0.5195]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5034]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4977]], dtype=torch.float64)\n",
      "tensor(2.9065e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9977, 0.4522, 0.9917, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4964]], dtype=torch.float64)\n",
      "tensor(8.0692e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9969, 0.4442, 0.9916, 0.5216]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5068]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4964]], dtype=torch.float64)\n",
      "tensor(6.6811e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9969, 0.4444, 0.9916, 0.5232]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5112]], dtype=torch.float64)\n",
      "tensor(1.5645e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9966, 0.4416, 0.9917, 0.5240]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5112]], dtype=torch.float64)\n",
      "tensor(1.3302e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5018, 0.9917, 0.5272]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5112]], dtype=torch.float64)\n",
      "tensor(1.5568e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 544/665 [00:06<00:01, 85.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4995, 0.9914, 0.5289]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5092]], dtype=torch.float64)\n",
      "tensor(1.8951e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4921, 0.9913, 0.5268]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5092]], dtype=torch.float64)\n",
      "tensor(1.4919e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4902, 0.9911, 0.5219]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5090]], dtype=torch.float64)\n",
      "tensor(2.7230e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4884, 0.9911, 0.5246]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5090]], dtype=torch.float64)\n",
      "tensor(1.5704e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4874, 0.9910, 0.5279]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5117]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5090]], dtype=torch.float64)\n",
      "tensor(1.8492e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4860, 0.9910, 0.5280]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5119]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5066]], dtype=torch.float64)\n",
      "tensor(2.8774e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.4768, 0.9910, 0.5281]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5127]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5066]], dtype=torch.float64)\n",
      "tensor(3.2854e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4756, 0.9911, 0.5248]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5086]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5029]], dtype=torch.float64)\n",
      "tensor(3.0606e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4750, 0.9912, 0.5238]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5075]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5029]], dtype=torch.float64)\n",
      "tensor(2.4433e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.4654, 0.9914, 0.5221]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5029]], dtype=torch.float64)\n",
      "tensor(1.7888e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.4648, 0.9916, 0.5220]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5004]], dtype=torch.float64)\n",
      "tensor(2.8396e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.4565, 0.9917, 0.5229]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5004]], dtype=torch.float64)\n",
      "tensor(3.9095e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9979, 0.4544, 0.9918, 0.5222]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5004]], dtype=torch.float64)\n",
      "tensor(3.3526e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9979, 0.4545, 0.9918, 0.5211]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5004]], dtype=torch.float64)\n",
      "tensor(2.5059e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 562/665 [00:06<00:01, 80.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9980, 0.4549, 0.9918, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5078]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5004]], dtype=torch.float64)\n",
      "tensor(4.0204e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9978, 0.4529, 0.9918, 0.5190]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5155]], dtype=torch.float64)\n",
      "tensor(9.2204e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5107, 0.9918, 0.5207]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5010]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5155]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5082, 0.9918, 0.5303]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5126]], dtype=torch.float64)\n",
      "tensor(1.5106e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.5088, 0.9918, 0.5229]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5126]], dtype=torch.float64)\n",
      "tensor(5.1357e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4990, 0.9917, 0.5238]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5126]], dtype=torch.float64)\n",
      "tensor(3.7249e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4987, 0.9913, 0.5269]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5100]], dtype=torch.float64)\n",
      "tensor(1.4787e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4892, 0.9910, 0.5241]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5100]], dtype=torch.float64)\n",
      "tensor(1.9926e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4877, 0.9911, 0.5256]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5100]], dtype=torch.float64)\n",
      "tensor(1.5304e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4867, 0.9911, 0.5277]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5115]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5090]], dtype=torch.float64)\n",
      "tensor(1.7892e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4854, 0.9910, 0.5295]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5138]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5090]], dtype=torch.float64)\n",
      "tensor(2.6515e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4847, 0.9910, 0.5306]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5154]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5066]], dtype=torch.float64)\n",
      "tensor(5.3591e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4746, 0.9910, 0.5289]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5066]], dtype=torch.float64)\n",
      "tensor(4.0881e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4742, 0.9912, 0.5262]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5049]], dtype=torch.float64)\n",
      "tensor(2.9671e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4742, 0.9913, 0.5247]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5086]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5049]], dtype=torch.float64)\n",
      "tensor(2.0706e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4635, 0.9915, 0.5231]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5049]], dtype=torch.float64)\n",
      "tensor(1.6260e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4637, 0.9917, 0.5244]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5030]], dtype=torch.float64)\n",
      "tensor(3.1098e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9978, 0.4526, 0.9919, 0.5215]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5030]], dtype=torch.float64)\n",
      "tensor(1.7251e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 580/665 [00:06<00:01, 81.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9978, 0.4529, 0.9920, 0.5209]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5053]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4991]], dtype=torch.float64)\n",
      "tensor(3.1743e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9979, 0.4537, 0.9920, 0.5230]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5079]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4991]], dtype=torch.float64)\n",
      "tensor(5.2010e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9966, 0.4420, 0.9919, 0.5209]], dtype=torch.float64)\n",
      "tensor([[0.9951, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.4991]], dtype=torch.float64)\n",
      "tensor(3.6861e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9967, 0.4430, 0.9918, 0.5212]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5132]], dtype=torch.float64)\n",
      "tensor(3.5171e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4986, 0.9918, 0.5234]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5052]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5132]], dtype=torch.float64)\n",
      "tensor(4.5761e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4984, 0.9918, 0.5254]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5078]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5139]], dtype=torch.float64)\n",
      "tensor(3.2904e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4967, 0.9917, 0.5233]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5053]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5139]], dtype=torch.float64)\n",
      "tensor(5.1315e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4967, 0.9917, 0.5233]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5053]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5139]], dtype=torch.float64)\n",
      "tensor(5.1315e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4953, 0.9914, 0.5275]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5139]], dtype=torch.float64)\n",
      "tensor(1.9803e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4949, 0.9911, 0.5264]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5116]], dtype=torch.float64)\n",
      "tensor(1.7620e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4854, 0.9909, 0.5239]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5068]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5116]], dtype=torch.float64)\n",
      "tensor(2.6046e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4846, 0.9910, 0.5262]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5092]], dtype=torch.float64)\n",
      "tensor(1.4754e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4833, 0.9911, 0.5307]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5155]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5092]], dtype=torch.float64)\n",
      "tensor(3.4935e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4736, 0.9911, 0.5282]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5092]], dtype=torch.float64)\n",
      "tensor(2.1997e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.4720, 0.9910, 0.5284]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5135]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5067]], dtype=torch.float64)\n",
      "tensor(3.7096e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.4630, 0.9911, 0.5272]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5067]], dtype=torch.float64)\n",
      "tensor(3.0709e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 598/665 [00:06<00:00, 81.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9986, 0.4630, 0.9913, 0.5258]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5067]], dtype=torch.float64)\n",
      "tensor(2.1695e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9985, 0.4611, 0.9915, 0.5257]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5054]], dtype=torch.float64)\n",
      "tensor(2.7686e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9985, 0.4618, 0.9916, 0.5263]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5115]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5054]], dtype=torch.float64)\n",
      "tensor(3.2167e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.4630, 0.9918, 0.5242]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5040]], dtype=torch.float64)\n",
      "tensor(2.4357e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9975, 0.4505, 0.9920, 0.5237]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5040]], dtype=torch.float64)\n",
      "tensor(2.5240e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9977, 0.4520, 0.9921, 0.5231]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5082]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5040]], dtype=torch.float64)\n",
      "tensor(2.1084e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9974, 0.4496, 0.9921, 0.5234]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5172]], dtype=torch.float64)\n",
      "tensor(4.7838e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5038, 0.9920, 0.5243]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5172]], dtype=torch.float64)\n",
      "tensor(7.5593e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5018, 0.9919, 0.5338]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5150]], dtype=torch.float64)\n",
      "tensor(1.9982e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4929, 0.9919, 0.5271]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5103]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5150]], dtype=torch.float64)\n",
      "tensor(2.5174e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4918, 0.9918, 0.5231]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5053]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5144]], dtype=torch.float64)\n",
      "tensor(5.4833e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4910, 0.9915, 0.5255]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5085]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5144]], dtype=torch.float64)\n",
      "tensor(3.1765e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.4808, 0.9912, 0.5281]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5144]], dtype=torch.float64)\n",
      "tensor(1.6224e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.4804, 0.9908, 0.5244]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5078]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5141]], dtype=torch.float64)\n",
      "tensor(3.4320e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.4803, 0.9910, 0.5267]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5141]], dtype=torch.float64)\n",
      "tensor(2.0177e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9995, 0.4786, 0.9911, 0.5330]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5141]], dtype=torch.float64)\n",
      "tensor(2.6279e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 607/665 [00:06<00:00, 78.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.4787, 0.9911, 0.5344]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5206]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5111]], dtype=torch.float64)\n",
      "tensor(5.9287e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4687, 0.9911, 0.5348]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5219]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5111]], dtype=torch.float64)\n",
      "tensor(7.2564e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9989, 0.4670, 0.9912, 0.5289]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5144]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5085]], dtype=torch.float64)\n",
      "tensor(3.1537e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9982, 0.4572, 0.9912, 0.5296]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5160]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5085]], dtype=torch.float64)\n",
      "tensor(4.2147e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9980, 0.4550, 0.9914, 0.5257]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5112]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5085]], dtype=torch.float64)\n",
      "tensor(1.6981e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.4564, 0.9916, 0.5271]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5067]], dtype=torch.float64)\n",
      "tensor(3.2486e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9972, 0.4471, 0.9918, 0.5281]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5149]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5067]], dtype=torch.float64)\n",
      "tensor(4.6046e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9969, 0.4444, 0.9919, 0.5255]], dtype=torch.float64)\n",
      "tensor([[0.9950, 0.5118]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5174]], dtype=torch.float64)\n",
      "tensor(2.7915e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4968, 0.9919, 0.5292]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5127]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5174]], dtype=torch.float64)\n",
      "tensor(2.5155e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4948, 0.9918, 0.5362]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5217]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5174]], dtype=torch.float64)\n",
      "tensor(2.4528e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4937, 0.9917, 0.5381]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5243]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5170]], dtype=torch.float64)\n",
      "tensor(4.2000e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4924, 0.9918, 0.5384]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5247]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5170]], dtype=torch.float64)\n",
      "tensor(4.4958e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4924, 0.9918, 0.5301]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5160]], dtype=torch.float64)\n",
      "tensor(1.6107e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4815, 0.9918, 0.5274]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5116]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5160]], dtype=torch.float64)\n",
      "tensor(2.3802e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 626/665 [00:07<00:00, 82.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9997, 0.4815, 0.9915, 0.5277]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5119]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5160]], dtype=torch.float64)\n",
      "tensor(2.2507e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4817, 0.9912, 0.5339]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5197]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5145]], dtype=torch.float64)\n",
      "tensor(2.8427e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.4700, 0.9910, 0.5247]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5145]], dtype=torch.float64)\n",
      "tensor(2.9612e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.4711, 0.9909, 0.5231]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5145]], dtype=torch.float64)\n",
      "tensor(4.4207e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9982, 0.4582, 0.9910, 0.5282]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5145]], dtype=torch.float64)\n",
      "tensor(1.3933e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.4587, 0.9911, 0.5350]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5228]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5145]], dtype=torch.float64)\n",
      "tensor(4.8750e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9984, 0.4601, 0.9911, 0.5346]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5222]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5149]], dtype=torch.float64)\n",
      "tensor(4.0929e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9982, 0.4577, 0.9912, 0.5335]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5210]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5149]], dtype=torch.float64)\n",
      "tensor(3.2307e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9984, 0.4598, 0.9912, 0.5345]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5194]], dtype=torch.float64)\n",
      "tensor(1.7973e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5017, 0.9913, 0.5351]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5198]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5194]], dtype=torch.float64)\n",
      "tensor(1.5586e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.5008, 0.9914, 0.5400]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5194]], dtype=torch.float64)\n",
      "tensor(3.9022e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4993, 0.9915, 0.5401]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5264]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5179]], dtype=torch.float64)\n",
      "tensor(5.1754e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4884, 0.9916, 0.5387]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5254]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5179]], dtype=torch.float64)\n",
      "tensor(4.3403e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4872, 0.9917, 0.5387]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5254]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5182]], dtype=torch.float64)\n",
      "tensor(4.1589e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4765, 0.9917, 0.5374]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5247]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5182]], dtype=torch.float64)\n",
      "tensor(3.5698e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9994, 0.4759, 0.9917, 0.5392]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5269]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5182]], dtype=torch.float64)\n",
      "tensor(5.3258e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4741, 0.9918, 0.5391]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5270]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5187]], dtype=torch.float64)\n",
      "tensor(4.9125e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4644, 0.9918, 0.5300]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5160]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5187]], dtype=torch.float64)\n",
      "tensor(1.7111e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 644/665 [00:07<00:00, 84.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9986, 0.4626, 0.9918, 0.5261]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5135]], dtype=torch.float64)\n",
      "tensor(1.5626e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4637, 0.9917, 0.5258]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5135]], dtype=torch.float64)\n",
      "tensor(1.7138e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9988, 0.4653, 0.9913, 0.5289]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5146]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5135]], dtype=torch.float64)\n",
      "tensor(1.4517e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4636, 0.9913, 0.5253]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5101]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5151]], dtype=torch.float64)\n",
      "tensor(2.6446e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9982, 0.4577, 0.9909, 0.5228]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5151]], dtype=torch.float64)\n",
      "tensor(4.4314e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9984, 0.4605, 0.9909, 0.5244]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5091]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5208]], dtype=torch.float64)\n",
      "tensor(8.1645e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4961, 0.9911, 0.5319]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5162]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5208]], dtype=torch.float64)\n",
      "tensor(2.5946e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4941, 0.9912, 0.5376]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5236]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5208]], dtype=torch.float64)\n",
      "tensor(1.9514e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4929, 0.9913, 0.5416]], dtype=torch.float64)\n",
      "tensor([[0.9944, 0.5288]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5199]], dtype=torch.float64)\n",
      "tensor(5.5084e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4819, 0.9912, 0.5404]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5280]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5199]], dtype=torch.float64)\n",
      "tensor(4.8159e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9996, 0.4812, 0.9912, 0.5398]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5202]], dtype=torch.float64)\n",
      "tensor(4.0395e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.4692, 0.9912, 0.5398]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5282]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5202]], dtype=torch.float64)\n",
      "tensor(4.6847e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4688, 0.9914, 0.5418]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5202]], dtype=torch.float64)\n",
      "tensor(7.0047e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9990, 0.4692, 0.9915, 0.5404]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5290]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5211]], dtype=torch.float64)\n",
      "tensor(4.5405e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9980, 0.4559, 0.9917, 0.5422]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5322]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5211]], dtype=torch.float64)\n",
      "tensor(7.5873e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9981, 0.4568, 0.9917, 0.5395]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5287]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5136]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.4583, 0.9917, 0.5404]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5298]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5136]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 661/665 [00:07<00:00, 76.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9981, 0.4560, 0.9917, 0.5329]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5204]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5136]], dtype=torch.float64)\n",
      "tensor(3.6703e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9983, 0.4588, 0.9917, 0.5251]], dtype=torch.float64)\n",
      "tensor([[0.9949, 0.5102]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5220]], dtype=torch.float64)\n",
      "tensor(8.2458e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4986, 0.9919, 0.5222]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5220]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[1.0000, 0.4981, 0.9918, 0.5282]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5215]], dtype=torch.float64)\n",
      "tensor(6.5214e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4836, 0.9915, 0.5328]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5183]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5215]], dtype=torch.float64)\n",
      "tensor(1.9734e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4827, 0.9912, 0.5374]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5215]], dtype=torch.float64)\n",
      "tensor(1.8895e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9997, 0.4820, 0.9911, 0.5267]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5203]], dtype=torch.float64)\n",
      "tensor(6.2080e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4733, 0.9907, 0.5212]], dtype=torch.float64)\n",
      "tensor([[0.9947, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5203]], dtype=torch.float64)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.4716, 0.9908, 0.5256]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5217]], dtype=torch.float64)\n",
      "tensor(8.4742e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9992, 0.4723, 0.9910, 0.5329]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5192]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5217]], dtype=torch.float64)\n",
      "tensor(1.7917e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9993, 0.4730, 0.9911, 0.5416]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5302]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5217]], dtype=torch.float64)\n",
      "tensor(5.0653e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9991, 0.4708, 0.9912, 0.5418]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5238]], dtype=torch.float64)\n",
      "tensor(3.9082e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9986, 0.4621, 0.9913, 0.5419]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5313]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5238]], dtype=torch.float64)\n",
      "tensor(4.3390e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9987, 0.4635, 0.9913, 0.5432]], dtype=torch.float64)\n",
      "tensor([[0.9945, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5112]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 665/665 [00:07<00:00, 86.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9976, 0.4507, 0.9915, 0.5425]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5112]], dtype=torch.float64)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9979, 0.4540, 0.9916, 0.5316]], dtype=torch.float64)\n",
      "tensor([[0.9948, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5112]], dtype=torch.float64)\n",
      "tensor(4.2988e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9999, 0.4886, 0.9917, 0.5317]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5247]], dtype=torch.float64)\n",
      "tensor(4.8167e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[0.9998, 0.4876, 0.9917, 0.5316]], dtype=torch.float64)\n",
      "tensor([[0.9946, 0.5164]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.0000, 0.5247]], dtype=torch.float64)\n",
      "tensor(4.9098e-05, grad_fn=<MseLossBackward0>)\n",
      "Final val loss: 0.0014308121466288675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load file and check MSELoss\n",
    "model = TransformerModel(num_lidar_features, num_non_lidar_features, num_actions)\n",
    "model.load_state_dict(torch.load('transformer_model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "device = 'cpu'\n",
    "\n",
    "# take world idx 0 as example\n",
    "dataset = KULBarnDataset(df[df['world_idx'] == 0], \"val\")\n",
    "print(len(dataset))\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "final_val_loss = test_model(model, loader, loss_fn)\n",
    "print(\"Final val loss:\", final_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hydra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhydra\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OmegaConf\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hydra'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hydra\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "import tqdm\n",
    "import shutil\n",
    "from diffusion_policy.policy.diffusion_unet_lowdim_policy import DiffusionUnetLowdimPolicy\n",
    "from diffusion_policy.workspace.train_diffusion_unet_lowdim_workspace import TrainDiffusionUnetLowdimWorkspace\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusion_policy.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLowdimDataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'diffusion_policy.dataset'"
     ]
    }
   ],
   "source": [
    "from diffusion_policy.dataset.base_dataset import BaseLowdimDataset\n",
    "from typing import Dict\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.common.replay_buffer import ReplayBuffer\n",
    "from diffusion_policy.common.sampler import (\n",
    "    SequenceSampler, get_val_mask, downsample_mask)\n",
    "from diffusion_policy.model.common.normalizer import LinearNormalizer\n",
    "from diffusion_policy.dataset.base_dataset import BaseLowdimDataset\n",
    "\n",
    "\n",
    "class KULBarnDiffusionDataset(BaseLowdimDataset):\n",
    "    def __init__(self, df, horizon=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = df\n",
    "        self.get_local_goal()\n",
    "\n",
    "        self.data = self.data.drop(columns=[\n",
    "            'timestep', 'actual_time', 'optimal_time', \n",
    "            'pos_x', 'pos_y', 'pose_heading', 'goal_x', 'goal_y', 'success'\n",
    "        ])\n",
    "        \n",
    "        self.data = pd.DataFrame(self.data, columns=self.data.columns)\n",
    "        self.horizon = horizon\n",
    "\n",
    "        # Process data columns\n",
    "        self.lidar_cols = [col for col in self.data.columns if 'lidar' in col]\n",
    "        self.actions_cols = [col for col in self.data.columns if 'cmd' in col]\n",
    "        self.non_lidar_cols = [col for col in self.data.columns if col not in self.lidar_cols and col not in self.actions_cols and col != 'world_idx']\n",
    "\n",
    "        self.lidar_data = self.data[self.lidar_cols].values\n",
    "        self.non_lidar_data = self.data[self.non_lidar_cols].values\n",
    "        self.actions_data = self.data[self.actions_cols].values\n",
    "\n",
    "        print(\"Lidar Columns:\", self.lidar_cols)\n",
    "        print(\"Non Lidar Columns:\", self.non_lidar_cols)\n",
    "        print(\"Action Columns:\", self.actions_cols)     \n",
    "\n",
    "        self.grouped_data = self.data.groupby(self.data['world_idx'])\n",
    "        self.horizon = horizon\n",
    "        path_lengths = [len(group) for name, group in self.grouped_data]\n",
    "        self.indices = self.make_indices(path_lengths, horizon)\n",
    "\n",
    "    def get_local_goal(self):\n",
    "        x = self.data['pos_x']\n",
    "        y = self.data['pos_y']\n",
    "        theta = self.data['pose_heading']\n",
    "        goal_x = self.data['goal_x']\n",
    "        goal_y = self.data['goal_y']\n",
    "        self.data['local_x'] = (goal_x - x) * np.cos(theta) + (goal_y - y) * np.sin(theta)\n",
    "        self.data['local_y'] = -(goal_x - x) * np.sin(theta) + (goal_y - y) * np.cos(theta)\n",
    "\n",
    "    def make_indices(self, path_lengths, horizon):\n",
    "        indices = []\n",
    "        for i, path_length in enumerate(path_lengths):\n",
    "            max_start = path_length - horizon\n",
    "            for start in range(max_start):\n",
    "                end = start + horizon\n",
    "                indices.append((i, start, end))\n",
    "        indices = np.array(indices)\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        idx = self.indices[idx]\n",
    "        start = idx[1]\n",
    "        end = idx[2]\n",
    "\n",
    "        data = {\n",
    "            'obs': self.lidar_data[start:end],\n",
    "            'cond': self.non_lidar_data[start:end],\n",
    "            'action': self.actions_data[start:end],\n",
    "        }\n",
    "        torch_data = dict_apply(data, torch.from_numpy)\n",
    "        return torch_data\n",
    "\n",
    "    def get_normalizer(self, mode='limits', **kwargs):\n",
    "        normalizer = LinearNormalizer()\n",
    "        # train it in using self.data as a dictionary\n",
    "        data_dict = {\n",
    "            'obs': self.lidar_data,\n",
    "            'cond': self.non_lidar_data,\n",
    "            'action': self.actions_data\n",
    "        }\n",
    "        normalizer.fit(data=data_dict, mode=mode, **kwargs)\n",
    "        return normalizer\n",
    "\n",
    "    def get_all_actions(self) -> torch.Tensor:\n",
    "        return torch.from_numpy(self.actions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lidar Columns: ['lidar_0', 'lidar_1', 'lidar_2', 'lidar_3', 'lidar_4', 'lidar_5', 'lidar_6', 'lidar_7', 'lidar_8', 'lidar_9', 'lidar_10', 'lidar_11', 'lidar_12', 'lidar_13', 'lidar_14', 'lidar_15', 'lidar_16', 'lidar_17', 'lidar_18', 'lidar_19', 'lidar_20', 'lidar_21', 'lidar_22', 'lidar_23', 'lidar_24', 'lidar_25', 'lidar_26', 'lidar_27', 'lidar_28', 'lidar_29', 'lidar_30', 'lidar_31', 'lidar_32', 'lidar_33', 'lidar_34', 'lidar_35', 'lidar_36', 'lidar_37', 'lidar_38', 'lidar_39', 'lidar_40', 'lidar_41', 'lidar_42', 'lidar_43', 'lidar_44', 'lidar_45', 'lidar_46', 'lidar_47', 'lidar_48', 'lidar_49', 'lidar_50', 'lidar_51', 'lidar_52', 'lidar_53', 'lidar_54', 'lidar_55', 'lidar_56', 'lidar_57', 'lidar_58', 'lidar_59', 'lidar_60', 'lidar_61', 'lidar_62', 'lidar_63', 'lidar_64', 'lidar_65', 'lidar_66', 'lidar_67', 'lidar_68', 'lidar_69', 'lidar_70', 'lidar_71', 'lidar_72', 'lidar_73', 'lidar_74', 'lidar_75', 'lidar_76', 'lidar_77', 'lidar_78', 'lidar_79', 'lidar_80', 'lidar_81', 'lidar_82', 'lidar_83', 'lidar_84', 'lidar_85', 'lidar_86', 'lidar_87', 'lidar_88', 'lidar_89', 'lidar_90', 'lidar_91', 'lidar_92', 'lidar_93', 'lidar_94', 'lidar_95', 'lidar_96', 'lidar_97', 'lidar_98', 'lidar_99', 'lidar_100', 'lidar_101', 'lidar_102', 'lidar_103', 'lidar_104', 'lidar_105', 'lidar_106', 'lidar_107', 'lidar_108', 'lidar_109', 'lidar_110', 'lidar_111', 'lidar_112', 'lidar_113', 'lidar_114', 'lidar_115', 'lidar_116', 'lidar_117', 'lidar_118', 'lidar_119', 'lidar_120', 'lidar_121', 'lidar_122', 'lidar_123', 'lidar_124', 'lidar_125', 'lidar_126', 'lidar_127', 'lidar_128', 'lidar_129', 'lidar_130', 'lidar_131', 'lidar_132', 'lidar_133', 'lidar_134', 'lidar_135', 'lidar_136', 'lidar_137', 'lidar_138', 'lidar_139', 'lidar_140', 'lidar_141', 'lidar_142', 'lidar_143', 'lidar_144', 'lidar_145', 'lidar_146', 'lidar_147', 'lidar_148', 'lidar_149', 'lidar_150', 'lidar_151', 'lidar_152', 'lidar_153', 'lidar_154', 'lidar_155', 'lidar_156', 'lidar_157', 'lidar_158', 'lidar_159', 'lidar_160', 'lidar_161', 'lidar_162', 'lidar_163', 'lidar_164', 'lidar_165', 'lidar_166', 'lidar_167', 'lidar_168', 'lidar_169', 'lidar_170', 'lidar_171', 'lidar_172', 'lidar_173', 'lidar_174', 'lidar_175', 'lidar_176', 'lidar_177', 'lidar_178', 'lidar_179', 'lidar_180', 'lidar_181', 'lidar_182', 'lidar_183', 'lidar_184', 'lidar_185', 'lidar_186', 'lidar_187', 'lidar_188', 'lidar_189', 'lidar_190', 'lidar_191', 'lidar_192', 'lidar_193', 'lidar_194', 'lidar_195', 'lidar_196', 'lidar_197', 'lidar_198', 'lidar_199', 'lidar_200', 'lidar_201', 'lidar_202', 'lidar_203', 'lidar_204', 'lidar_205', 'lidar_206', 'lidar_207', 'lidar_208', 'lidar_209', 'lidar_210', 'lidar_211', 'lidar_212', 'lidar_213', 'lidar_214', 'lidar_215', 'lidar_216', 'lidar_217', 'lidar_218', 'lidar_219', 'lidar_220', 'lidar_221', 'lidar_222', 'lidar_223', 'lidar_224', 'lidar_225', 'lidar_226', 'lidar_227', 'lidar_228', 'lidar_229', 'lidar_230', 'lidar_231', 'lidar_232', 'lidar_233', 'lidar_234', 'lidar_235', 'lidar_236', 'lidar_237', 'lidar_238', 'lidar_239', 'lidar_240', 'lidar_241', 'lidar_242', 'lidar_243', 'lidar_244', 'lidar_245', 'lidar_246', 'lidar_247', 'lidar_248', 'lidar_249', 'lidar_250', 'lidar_251', 'lidar_252', 'lidar_253', 'lidar_254', 'lidar_255', 'lidar_256', 'lidar_257', 'lidar_258', 'lidar_259', 'lidar_260', 'lidar_261', 'lidar_262', 'lidar_263', 'lidar_264', 'lidar_265', 'lidar_266', 'lidar_267', 'lidar_268', 'lidar_269', 'lidar_270', 'lidar_271', 'lidar_272', 'lidar_273', 'lidar_274', 'lidar_275', 'lidar_276', 'lidar_277', 'lidar_278', 'lidar_279', 'lidar_280', 'lidar_281', 'lidar_282', 'lidar_283', 'lidar_284', 'lidar_285', 'lidar_286', 'lidar_287', 'lidar_288', 'lidar_289', 'lidar_290', 'lidar_291', 'lidar_292', 'lidar_293', 'lidar_294', 'lidar_295', 'lidar_296', 'lidar_297', 'lidar_298', 'lidar_299', 'lidar_300', 'lidar_301', 'lidar_302', 'lidar_303', 'lidar_304', 'lidar_305', 'lidar_306', 'lidar_307', 'lidar_308', 'lidar_309', 'lidar_310', 'lidar_311', 'lidar_312', 'lidar_313', 'lidar_314', 'lidar_315', 'lidar_316', 'lidar_317', 'lidar_318', 'lidar_319', 'lidar_320', 'lidar_321', 'lidar_322', 'lidar_323', 'lidar_324', 'lidar_325', 'lidar_326', 'lidar_327', 'lidar_328', 'lidar_329', 'lidar_330', 'lidar_331', 'lidar_332', 'lidar_333', 'lidar_334', 'lidar_335', 'lidar_336', 'lidar_337', 'lidar_338', 'lidar_339', 'lidar_340', 'lidar_341', 'lidar_342', 'lidar_343', 'lidar_344', 'lidar_345', 'lidar_346', 'lidar_347', 'lidar_348', 'lidar_349', 'lidar_350', 'lidar_351', 'lidar_352', 'lidar_353', 'lidar_354', 'lidar_355', 'lidar_356', 'lidar_357', 'lidar_358', 'lidar_359', 'lidar_360', 'lidar_361', 'lidar_362', 'lidar_363', 'lidar_364', 'lidar_365', 'lidar_366', 'lidar_367', 'lidar_368', 'lidar_369', 'lidar_370', 'lidar_371', 'lidar_372', 'lidar_373', 'lidar_374', 'lidar_375', 'lidar_376', 'lidar_377', 'lidar_378', 'lidar_379', 'lidar_380', 'lidar_381', 'lidar_382', 'lidar_383', 'lidar_384', 'lidar_385', 'lidar_386', 'lidar_387', 'lidar_388', 'lidar_389', 'lidar_390', 'lidar_391', 'lidar_392', 'lidar_393', 'lidar_394', 'lidar_395', 'lidar_396', 'lidar_397', 'lidar_398', 'lidar_399', 'lidar_400', 'lidar_401', 'lidar_402', 'lidar_403', 'lidar_404', 'lidar_405', 'lidar_406', 'lidar_407', 'lidar_408', 'lidar_409', 'lidar_410', 'lidar_411', 'lidar_412', 'lidar_413', 'lidar_414', 'lidar_415', 'lidar_416', 'lidar_417', 'lidar_418', 'lidar_419', 'lidar_420', 'lidar_421', 'lidar_422', 'lidar_423', 'lidar_424', 'lidar_425', 'lidar_426', 'lidar_427', 'lidar_428', 'lidar_429', 'lidar_430', 'lidar_431', 'lidar_432', 'lidar_433', 'lidar_434', 'lidar_435', 'lidar_436', 'lidar_437', 'lidar_438', 'lidar_439', 'lidar_440', 'lidar_441', 'lidar_442', 'lidar_443', 'lidar_444', 'lidar_445', 'lidar_446', 'lidar_447', 'lidar_448', 'lidar_449', 'lidar_450', 'lidar_451', 'lidar_452', 'lidar_453', 'lidar_454', 'lidar_455', 'lidar_456', 'lidar_457', 'lidar_458', 'lidar_459', 'lidar_460', 'lidar_461', 'lidar_462', 'lidar_463', 'lidar_464', 'lidar_465', 'lidar_466', 'lidar_467', 'lidar_468', 'lidar_469', 'lidar_470', 'lidar_471', 'lidar_472', 'lidar_473', 'lidar_474', 'lidar_475', 'lidar_476', 'lidar_477', 'lidar_478', 'lidar_479', 'lidar_480', 'lidar_481', 'lidar_482', 'lidar_483', 'lidar_484', 'lidar_485', 'lidar_486', 'lidar_487', 'lidar_488', 'lidar_489', 'lidar_490', 'lidar_491', 'lidar_492', 'lidar_493', 'lidar_494', 'lidar_495', 'lidar_496', 'lidar_497', 'lidar_498', 'lidar_499', 'lidar_500', 'lidar_501', 'lidar_502', 'lidar_503', 'lidar_504', 'lidar_505', 'lidar_506', 'lidar_507', 'lidar_508', 'lidar_509', 'lidar_510', 'lidar_511', 'lidar_512', 'lidar_513', 'lidar_514', 'lidar_515', 'lidar_516', 'lidar_517', 'lidar_518', 'lidar_519', 'lidar_520', 'lidar_521', 'lidar_522', 'lidar_523', 'lidar_524', 'lidar_525', 'lidar_526', 'lidar_527', 'lidar_528', 'lidar_529', 'lidar_530', 'lidar_531', 'lidar_532', 'lidar_533', 'lidar_534', 'lidar_535', 'lidar_536', 'lidar_537', 'lidar_538', 'lidar_539', 'lidar_540', 'lidar_541', 'lidar_542', 'lidar_543', 'lidar_544', 'lidar_545', 'lidar_546', 'lidar_547', 'lidar_548', 'lidar_549', 'lidar_550', 'lidar_551', 'lidar_552', 'lidar_553', 'lidar_554', 'lidar_555', 'lidar_556', 'lidar_557', 'lidar_558', 'lidar_559', 'lidar_560', 'lidar_561', 'lidar_562', 'lidar_563', 'lidar_564', 'lidar_565', 'lidar_566', 'lidar_567', 'lidar_568', 'lidar_569', 'lidar_570', 'lidar_571', 'lidar_572', 'lidar_573', 'lidar_574', 'lidar_575', 'lidar_576', 'lidar_577', 'lidar_578', 'lidar_579', 'lidar_580', 'lidar_581', 'lidar_582', 'lidar_583', 'lidar_584', 'lidar_585', 'lidar_586', 'lidar_587', 'lidar_588', 'lidar_589', 'lidar_590', 'lidar_591', 'lidar_592', 'lidar_593', 'lidar_594', 'lidar_595', 'lidar_596', 'lidar_597', 'lidar_598', 'lidar_599', 'lidar_600', 'lidar_601', 'lidar_602', 'lidar_603', 'lidar_604', 'lidar_605', 'lidar_606', 'lidar_607', 'lidar_608', 'lidar_609', 'lidar_610', 'lidar_611', 'lidar_612', 'lidar_613', 'lidar_614', 'lidar_615', 'lidar_616', 'lidar_617', 'lidar_618', 'lidar_619', 'lidar_620', 'lidar_621', 'lidar_622', 'lidar_623', 'lidar_624', 'lidar_625', 'lidar_626', 'lidar_627', 'lidar_628', 'lidar_629', 'lidar_630', 'lidar_631', 'lidar_632', 'lidar_633', 'lidar_634', 'lidar_635', 'lidar_636', 'lidar_637', 'lidar_638', 'lidar_639', 'lidar_640', 'lidar_641', 'lidar_642', 'lidar_643', 'lidar_644', 'lidar_645', 'lidar_646', 'lidar_647', 'lidar_648', 'lidar_649', 'lidar_650', 'lidar_651', 'lidar_652', 'lidar_653', 'lidar_654', 'lidar_655', 'lidar_656', 'lidar_657', 'lidar_658', 'lidar_659', 'lidar_660', 'lidar_661', 'lidar_662', 'lidar_663', 'lidar_664', 'lidar_665', 'lidar_666', 'lidar_667', 'lidar_668', 'lidar_669', 'lidar_670', 'lidar_671', 'lidar_672', 'lidar_673', 'lidar_674', 'lidar_675', 'lidar_676', 'lidar_677', 'lidar_678', 'lidar_679', 'lidar_680', 'lidar_681', 'lidar_682', 'lidar_683', 'lidar_684', 'lidar_685', 'lidar_686', 'lidar_687', 'lidar_688', 'lidar_689', 'lidar_690', 'lidar_691', 'lidar_692', 'lidar_693', 'lidar_694', 'lidar_695', 'lidar_696', 'lidar_697', 'lidar_698', 'lidar_699', 'lidar_700', 'lidar_701', 'lidar_702', 'lidar_703', 'lidar_704', 'lidar_705', 'lidar_706', 'lidar_707', 'lidar_708', 'lidar_709', 'lidar_710', 'lidar_711', 'lidar_712', 'lidar_713', 'lidar_714', 'lidar_715', 'lidar_716', 'lidar_717', 'lidar_718', 'lidar_719']\n",
      "Non Lidar Columns: ['twist_linear', 'twist_angular', 'local_x', 'local_y']\n",
      "Action Columns: ['cmd_vel_linear', 'cmd_vel_angular']\n",
      "141127\n"
     ]
    }
   ],
   "source": [
    "train_dataset = KULBarnDiffusionDataset(train_df)\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "normalizer = train_dataset.get_normalizer()\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 720])\n",
      "torch.Size([1, 4, 4])\n",
      "torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    # print(batch)\n",
    "    print(batch['obs'].shape)\n",
    "    print(batch['cond'].shape)\n",
    "    print(batch['action'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_policy.policy.diffusion_unet_lidar_policy import DiffusionUnetLidarPolicy\n",
    "from diffusion_policy.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "obs_dim = batch['obs'].shape[-1]\n",
    "action_dim = batch['action'].shape[-1]\n",
    "input_dim = obs_dim + action_dim\n",
    "model = ConditionalUnet1D(input_dim=input_dim)\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='linear')\n",
    "horizon = 4\n",
    "policy = DiffusionUnetLidarPolicy(\n",
    "    model=model, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    horizon=horizon, \n",
    "    obs_dim=obs_dim, \n",
    "    action_dim=action_dim, \n",
    "    n_obs_steps=4,\n",
    "    n_action_steps=4,\n",
    "    pred_action_steps_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.set_normalizer(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/141127 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m---> 13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/raid/joshua/codes/mlda-barn-2024/train_imitation/diffusion_policy/diffusion_policy/policy/diffusion_unet_lidar_policy.py:242\u001b[0m, in \u001b[0;36mDiffusionUnetLidarPolicy.compute_loss\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    239\u001b[0m noisy_trajectory[condition_mask] \u001b[38;5;241m=\u001b[39m trajectory[condition_mask]\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Predict the noise residual, passing 'cond' as an additional conditioning input\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_trajectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlocal_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m pred_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_scheduler\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprediction_type\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/raid/joshua/codes/mlda-barn-2024/train_imitation/diffusion_policy/diffusion_policy/model/diffusion/conditional_unet1d.py:199\u001b[0m, in \u001b[0;36mConditionalUnet1D.forward\u001b[0;34m(self, sample, timestep, local_cond, global_cond, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m global_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_step_encoder(timesteps)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_cond \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     global_feature \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_cond\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# encode local features\u001b[39;00m\n\u001b[1;32m    204\u001b[0m h_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "losses = []\n",
    "save_loss_every = 1000\n",
    "total_loss = 0\n",
    "count = 0\n",
    "\n",
    "optimizer = optim.Adam(policy.model.parameters(), lr=5e-5)\n",
    "policy.model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        loss = policy.compute_loss(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        count += 1\n",
    "        if count >= save_loss_every:\n",
    "            curr_loss = total_loss / save_loss_every\n",
    "            print(\"Loss:\", curr_loss)\n",
    "            losses.append(curr_loss)\n",
    "            total_loss = 0\n",
    "            count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
